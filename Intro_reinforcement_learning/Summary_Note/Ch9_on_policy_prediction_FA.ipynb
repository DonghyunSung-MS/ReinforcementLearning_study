{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch.9 On-policy prediction with Approximation\n",
    "  \n",
    "This chpater describes how to move from tabular method to function approximation. Function approximation is a function that represent value function(action or state) with parameters. This can be weights in neural network or linear function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Value-function Approximation\n",
    "\n",
    "### 9.1.1 Recap\n",
    "\n",
    "#### Monte Carlo  \n",
    "MC updates value function based on episodic experience and Target becomes $G_t$(cummulative reward)\n",
    "#### TD(0)  \n",
    "TD(0) updates value function every step based on TD_target( $R^a_s + \\gamma V(s')$ )\n",
    "#### TD($\\lambda$)  \n",
    "TD($lambda$) also called n-step TD. Instead of one step look ahead, it views n-step and then calculate geometry sum of return for each step($G_t^\\lambda$). However, forward view is cumputationally high so in same manner, backward view is used with Eligibility traces\n",
    "\n",
    "### 9.1.2 Approximation\n",
    "\n",
    "We use various method(i.e upper 3) to update parameter vector $W$ in value function. Expecially in NN, multiple technique from supervised learning are used. In supervised learning, however, it assumes that all data to train the model(NN) is independent and identically distributed(i.i.d). On the other hand,in RL, data comes from experience is highly correlated. There are some try to break this correlation in the feild of RL such as experience replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 The Prediction Objective\n",
    "\n",
    "### on-policy distribution in episodic tasks\n",
    "* $h(s)$ denote the probability that an episode begins in each state $s$\n",
    "* $\\eta(s)$ denote the number of time steps spent in state $s$ in a single episode\n",
    "$$\\eta(s) = h(s)+\\sum_{\\bar{s}}\\eta(\\bar{s})\\sum_{a}\\pi(a|\\bar{s})p(s|\\bar{s},a) $$\n",
    "* prob of start at $s$  +  sum of prob $\\bar{s}\\rightarrow s$ \n",
    "* on policy distribution(without discounting)\n",
    "$$\\mu(s) = \\frac{\\eta(s)}{\\sum_{s}\\eta(s)},\\:\\: for\\: all\\: s \\in S $$\n",
    "\n",
    "### MSE objective\n",
    "* The prediction objective we choose in here is mean-square-error. This equation means that how prediction is made compared to true state value function following the policy$\\pi$. $\\mu(s)$ denotes probability of being in that state $s$.\n",
    "$$\\overline{VE}(w) \\approx \\sum_{s\\in S}\\mu(s)[v_{\\pi}(s)-\\hat{v}(s,w)]^2$$\n",
    "* The main objective is to reduce the predefined loss. However, the more complex the function, the harder to guarantee that obtatined parameter vector($w^*$) is global optimal. Sometimes, the objective function diverges with optimization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Stochastic-gradient and Semi-gradient Methods\n",
    "\n",
    "### SGD for MSE objective\n",
    "$$w_{t+1} = w_{t} - \\frac{1}{2}*\\alpha*\\nabla\\overline{VE} $$\n",
    "$$w_{t+1} = w_{t}+\\alpha*[v_{\\pi}(s)-\\hat{v}(s,w)]*\\nabla\\hat{v}(s,w)$$\n",
    "but we do not know the exact $v_{\\pi}(s)$ so we subsitute to others that we can calculate and predict. \n",
    "\n",
    "* MC gradient(unbiased high variance)- non-bootstrap\n",
    "* TD(0) gradient(biased low variance)- bootstrap(using current knowlege to estimate next value function)\n",
    "* TD( $\\lambda$ ) gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Linear objective representation\n",
    "It has shown that bootstrapping for non-linear approximation diverges.On the other hand, linear approximation proves that with bootstrapping, the function converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Feature Construction for Linear Methods\n",
    "\n",
    "### Polynomial Feature\n",
    "\n",
    "$$x_{i}(s) = Î ^{k}_{j=1}s^{c ji}_j$$\n",
    "\n",
    "### Fourier Basis\n",
    "* Fourier Basis is more efficient than polynomial feature in online-learning\n",
    "\n",
    "### Coarse Coding\n",
    "### Tile coding\n",
    "### Radial Basis Fuction(Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Non-Linear Function Approximation: ANN\n",
    "* Details in ch.16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
