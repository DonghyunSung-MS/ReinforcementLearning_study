{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "#### Bellman Optimality Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# set state\n",
    "import numpy as np\n",
    "nCols = 3\n",
    "nRows = 4\n",
    "nWalls = 1\n",
    "states = []\n",
    "for i in range(nCols*nRows-nWalls):\n",
    "    states.append(i)\n",
    "N_STATES = len(states)\n",
    "print(N_STATES)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1. -1. -1. -1.]\n",
      " [-1.  0.  0.  0.  0. -1.]\n",
      " [-1.  0. -1.  0.  0. -1.]\n",
      " [-1.  0.  0.  0.  0. -1.]\n",
      " [-1. -1. -1. -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# set map\n",
    "map = -np.ones((nCols+2,nRows+2))\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        map[i+1,j+1] = 0\n",
    "map[2,2] = -1 # add wall\n",
    "print(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set action\n",
    "actions = [0, 1, 2, 3]\n",
    "N_ACTIONS = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (1, 3), (1, 4), (2, 1), (2, 3), (2, 4), (3, 1), (3, 2), (3, 3), (3, 4)]\n",
      "[(0, -1), (-1, 0), (0, 1), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "# states -> location\n",
    "locations = []\n",
    "index = 0\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        if map[i+1,j+1]==0:\n",
    "            locations.append((i+1,j+1))\n",
    "            index = index + 1\n",
    "print(locations) # match index with states\n",
    "# action -> move\n",
    "move = [(0,-1),(-1,0),(0,1),(1,0)] # match index with actions\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set transition probability\n",
    "P = np.zeros((N_STATES,N_ACTIONS,N_STATES)) # P[S,A,S']\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        current_location = locations[s]\n",
    "        # heading collectly  ####################################################################################\n",
    "        next_location = (current_location[0] + move[a][0],current_location[1] + move[a][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.8\n",
    "        # left error ############################################################################################\n",
    "        next_location = (current_location[0] + move[a-1][0],current_location[1] + move[a-1][1])\n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        # right error ############################################################################################\n",
    "        next_location = (current_location[0] + move[(a+1)%4][0],current_location[1] + move[(a+1)%4][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02 -0.02 -0.02 -0.02]\n",
      " [-0.02 -0.02 -0.02 -0.02]\n",
      " [-0.02 -0.02 -0.02 -0.02]\n",
      " [ 1.    1.    1.    1.  ]\n",
      " [-0.02 -0.02 -0.02 -0.02]\n",
      " [-0.02 -0.02 -0.02 -0.02]\n",
      " [-1.   -1.   -1.   -1.  ]\n",
      " [-0.02 -0.02 -0.02 -0.02]\n",
      " [-0.02 -0.02 -0.02 -0.02]\n",
      " [-0.02 -0.02 -0.02 -0.02]\n",
      " [-0.02 -0.02 -0.02 -0.02]]\n"
     ]
    }
   ],
   "source": [
    "# rewards s,a ---  R(s,a)  ---> s'\n",
    "if True:\n",
    "    R = -0.02*np.ones((N_STATES,N_ACTIONS))\n",
    "else:\n",
    "    R = -0.5*np.ones((N_STATES,N_ACTIONS))\n",
    "R[3,:] = 1\n",
    "R[6,:] = -1\n",
    "print(R)\n",
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy : given state which action would u choose\n",
    "# assume that we know the policy\n",
    "bad_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "bad_policy[0,2] = 1\n",
    "bad_policy[1,2] = 1\n",
    "bad_policy[2,2] = 1\n",
    "bad_policy[3,2] = 1\n",
    "bad_policy[4,3] = 1\n",
    "bad_policy[5,2] = 1\n",
    "bad_policy[6,2] = 1\n",
    "bad_policy[7,2] = 1\n",
    "bad_policy[8,2] = 1\n",
    "bad_policy[9,2] = 1\n",
    "bad_policy[10,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = 0.25*np.ones((N_STATES,N_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "optimal_policy[0,2] = 1\n",
    "optimal_policy[1,2] = 1\n",
    "optimal_policy[2,2] = 1\n",
    "optimal_policy[3,2] = 1\n",
    "optimal_policy[4,1] = 1\n",
    "optimal_policy[5,1] = 1\n",
    "optimal_policy[6,1] = 1\n",
    "optimal_policy[7,1] = 1\n",
    "optimal_policy[8,0] = 1\n",
    "optimal_policy[9,0] = 1\n",
    "optimal_policy[10,0] = 1\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalWithNoise_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "ep = 0.1\n",
    "optimalWithNoise_policy[0,2] = 1\n",
    "optimalWithNoise_policy[1,2] = 1\n",
    "optimalWithNoise_policy[2,2] = 1\n",
    "optimalWithNoise_policy[3,2] = 1\n",
    "optimalWithNoise_policy[4,1] = 1\n",
    "optimalWithNoise_policy[5,1] = 1\n",
    "optimalWithNoise_policy[6,1] = 1\n",
    "optimalWithNoise_policy[7,1] = 1\n",
    "optimalWithNoise_policy[8,0] = 1\n",
    "optimalWithNoise_policy[9,0] = 1\n",
    "optimalWithNoise_policy[10,0] = 1\n",
    "optimalWithNoise_policy = optimalWithNoise_policy + (ep/4)*np.ones((N_STATES,N_ACTIONS))\n",
    "optimalWithNoise_policy = optimalWithNoise_policy / np.sum(optimalWithNoise_policy,axis = 1).reshape((N_STATES,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State value iteration with bellman optimality eqn\n",
    "#### optimal state value algorithm\n",
    "1. initialize with all zero\n",
    "1. for every state s v * (s) = max for a [R(s,a) + gamma * sum for s1(P(s,a,s1)*v * (s1)]\n",
    "1. using computed v * ,Fine optimal policy by solving\n",
    "    * optimal policy = argmax.a(q * (s,a))\n",
    "    * where q * (s,a) = R(s,a) + gamma * sum for s1 (P*v * (s1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85530117  0.89580324  0.93236641  1.          0.81969892  0.68749634\n",
      " -1.          0.78026128  0.74559468  0.70873821  0.49092193]\n",
      "[[ 0.82322354  0.83075787  0.85530117  0.80256088]\n",
      " [ 0.83476757  0.86645526  0.89580324  0.86645526]\n",
      " [ 0.84984258  0.90611872  0.93236641  0.71218162]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.79112222  0.81969892  0.79112222  0.76026732]\n",
      " [ 0.68696646  0.68749634 -0.64953064  0.5103828 ]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.75636299  0.78026128  0.72890705  0.74902668]\n",
      " [ 0.74559468  0.71792194  0.68894841  0.71792194]\n",
      " [ 0.70873821  0.64691224  0.50703739  0.66373581]\n",
      " [ 0.49092193 -0.69323365  0.31841144  0.48757652]]\n",
      "[2 2 2 0 1 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Vstar = np.zeros((N_STATES))\n",
    "Vstar[3] = 1\n",
    "Vstar[6] = -1\n",
    "\n",
    "num_iterations = 100\n",
    "historyVstar = np.zeros((num_iterations+1,N_STATES))\n",
    "historyVstar[:,3] = 1\n",
    "historyVstar[:,6] = -1\n",
    "# get optimal state value function\n",
    "for _ in range(num_iterations):\n",
    "    for s in range(N_STATES):\n",
    "        if (s!=3) and (s!=6):\n",
    "            Vstar[s] = max([R[s,a] + gamma*\\\n",
    "                            sum([P[s,a,s1]*Vstar[s1] \\\n",
    "                                 for s1 in range(N_STATES)])\\\n",
    "                                        for a in range(N_ACTIONS)])\n",
    "\n",
    "    historyVstar[_+1:]=Vstar\n",
    "    \n",
    "# get optimal policy from optimal state value function\n",
    "Qstar = np.zeros((N_STATES,N_ACTIONS))\n",
    "Qstar[3,:] = 1\n",
    "Qstar[6,:] = -1\n",
    "for s in range(N_STATES):\n",
    "        if (s!=3) and (s!=6):\n",
    "            for a in range(N_ACTIONS):\n",
    "                Qstar[s,a] = R[s,a] + gamma*sum([P[s,a,s1]*Vstar[s1] for s1 in range(N_STATES)])\n",
    "\n",
    "print(Vstar)\n",
    "print(Qstar)\n",
    "optimalPolicyFromV = np.argmax(Qstar,axis=1)\n",
    "\n",
    "print(optimalPolicyFromV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(historyVstar)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State value iteration with bellman optimality eqn\n",
    "#### optimal action value algorithm\n",
    "1. initialize with all zero\n",
    "1. for every state s,a\n",
    "    * q * (s,a) = [R(s,a) + gamma * sum for s1(P(s,a,s1)*max(q * (s1,a1))]\n",
    "1. Fine optimal policy by solving\n",
    "    * optimal policy = argmax.a(q * (s,a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opimtal Policy from Q\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Opimtal Policy from V\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Optimal Policy from previous code(deterministic)\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Q_star = np.zeros((N_STATES,N_ACTIONS))\n",
    "Q_star[3,:] = 1.\n",
    "Q_star[6, : ] = -1.\n",
    "for _ in range(num_iterations):\n",
    "    for s in range(N_STATES):\n",
    "        if (s!=3) and (s!=6):\n",
    "            for a in range(N_ACTIONS):\n",
    "                Q_star[s,a] = R[s,a] + gamma * \\\n",
    "                            sum([ P[s,a,s1] * \\\n",
    "                                max([Q_star[s1,a1] for a1 in range(N_ACTIONS)])\\\n",
    "                                for s1 in range(N_STATES)])\n",
    "    \n",
    "#print(Q_star)\n",
    "#print()\n",
    "optimalPolicyFromQ = np.argmax(Q_star,axis=1)\n",
    "print(\"Opimtal Policy from Q\")\n",
    "print(np.eye(4)[optimalPolicyFromQ])\n",
    "print()\n",
    "print(\"Opimtal Policy from V\")\n",
    "print(np.eye(4)[optimalPolicyFromV])\n",
    "print()\n",
    "print(\"Optimal Policy from previous code(deterministic)\")\n",
    "print(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
