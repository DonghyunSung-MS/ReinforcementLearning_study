{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Differance ( TD(0) )\n",
    "#### Incremental MC\n",
    "    * Reccurence formula of average calculation\n",
    "$$\\mu_{k} = \\mu_{k-1}+{{1}\\over{k}}(x_{k}-\\mu_{k-1}) $$\n",
    "    * MC(stationary)\n",
    "$$ V(S_{t})\\: \\leftarrow\\:V(S_{t})\\:+\\:{1\\over{N(S)}}\\:*\\:(G_{t}-V(S_{t}))$$\n",
    "    * MC(non-stationary)\n",
    "$$V(S_{t})\\: \\leftarrow\\:V(S_{t})\\:+\\:\\alpha\\:*\\:(G_{t}-V(S_{t}))$$\n",
    "#### TD(0)\n",
    "    * Instead of waiting utill simulation finishes, update value function step by step\n",
    "\n",
    "$$V(S_{t})\\: \\leftarrow\\:V(S_{t})\\:+\\:\\alpha\\:*\\:(R_{t+1}\\:+\\:\\gamma V(S_{t+1})-V(S_{t}))$$\n",
    "    * TD target is bias estimation of real $G_{t}$ and has low variance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code \n",
    "#### Grid World Enviroment setting\n",
    "* states, actions, transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set state\n",
    "import numpy as np\n",
    "nCols = 3\n",
    "nRows = 4\n",
    "nWalls = 1\n",
    "states = []\n",
    "for i in range(nCols*nRows-nWalls):\n",
    "    states.append(i)\n",
    "N_STATES = len(states)\n",
    "#print(N_STATES)\n",
    "#print(states)\n",
    "\n",
    "# set map\n",
    "map = -np.ones((nCols+2,nRows+2))\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        map[i+1,j+1] = 0\n",
    "map[2,2] = -1 # add wall\n",
    "#print(map)\n",
    "\n",
    "# set action\n",
    "actions = [0, 1, 2, 3]\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# states -> location\n",
    "locations = []\n",
    "index = 0\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        if map[i+1,j+1]==0:\n",
    "            locations.append((i+1,j+1))\n",
    "            index = index + 1\n",
    "#print(locations) # match index with states\n",
    "# action -> move\n",
    "move = [(0,-1),(-1,0),(0,1),(1,0)] # match index with actions\n",
    "#print(move)\n",
    "\n",
    "# set transition probability\n",
    "P = np.zeros((N_STATES,N_ACTIONS,N_STATES)) # P[S,A,S']\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        current_location = locations[s]\n",
    "        # heading collectly  ####################################################################################\n",
    "        next_location = (current_location[0] + move[a][0],current_location[1] + move[a][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.8\n",
    "        # left error ############################################################################################\n",
    "        next_location = (current_location[0] + move[a-1][0],current_location[1] + move[a-1][1])\n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        # right error ############################################################################################\n",
    "        next_location = (current_location[0] + move[(a+1)%4][0],current_location[1] + move[(a+1)%4][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        \n",
    "# rewards s,a ---  R(s,a)  ---> s'\n",
    "if True:\n",
    "    R = -0.02*np.ones((N_STATES,N_ACTIONS))\n",
    "else:\n",
    "    R = -0.5*np.ones((N_STATES,N_ACTIONS))\n",
    "R[3,:] = 1\n",
    "R[6,:] = -1\n",
    "#print(R)\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy : given state which action would u choose\n",
    "# assume that we know the policy\n",
    "bad_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "bad_policy[0,2] = 1\n",
    "bad_policy[1,2] = 1\n",
    "bad_policy[2,2] = 1\n",
    "bad_policy[3,2] = 1\n",
    "bad_policy[4,3] = 1\n",
    "bad_policy[5,2] = 1\n",
    "bad_policy[6,2] = 1\n",
    "bad_policy[7,2] = 1\n",
    "bad_policy[8,2] = 1\n",
    "bad_policy[9,2] = 1\n",
    "bad_policy[10,1] = 1\n",
    "\n",
    "random_policy = 0.25*np.ones((N_STATES,N_ACTIONS))\n",
    "\n",
    "optimal_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "optimal_policy[0,2] = 1\n",
    "optimal_policy[1,2] = 1\n",
    "optimal_policy[2,2] = 1\n",
    "optimal_policy[3,2] = 1\n",
    "optimal_policy[4,1] = 1\n",
    "optimal_policy[5,1] = 1\n",
    "optimal_policy[6,1] = 1\n",
    "optimal_policy[7,1] = 1\n",
    "optimal_policy[8,0] = 1\n",
    "optimal_policy[9,0] = 1\n",
    "optimal_policy[10,0] = 1\n",
    "#print(optimal_policy)\n",
    "\n",
    "optimalWithNoise_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "ep = 0.1\n",
    "optimalWithNoise_policy[0,2] = 1\n",
    "optimalWithNoise_policy[1,2] = 1\n",
    "optimalWithNoise_policy[2,2] = 1\n",
    "optimalWithNoise_policy[3,2] = 1\n",
    "optimalWithNoise_policy[4,1] = 1\n",
    "optimalWithNoise_policy[5,1] = 1\n",
    "optimalWithNoise_policy[6,1] = 1\n",
    "optimalWithNoise_policy[7,1] = 1\n",
    "optimalWithNoise_policy[8,0] = 1\n",
    "optimalWithNoise_policy[9,0] = 1\n",
    "optimalWithNoise_policy[10,0] = 1\n",
    "optimalWithNoise_policy = optimalWithNoise_policy + (ep/4)*np.ones((N_STATES,N_ACTIONS))\n",
    "optimalWithNoise_policy = optimalWithNoise_policy / np.sum(optimalWithNoise_policy,axis = 1).reshape((N_STATES,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.82866665  0.87919271  0.92157334  1.          0.79087506  0.57785063\n",
      " -1.          0.75080509  0.71718853  0.66761782  0.39857905]\n"
     ]
    }
   ],
   "source": [
    "## TD(0) for V\n",
    "\n",
    "## set Hyper parameters\n",
    "epoch = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "## set boundary condition\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1.0; #goal\n",
    "V[6] = -1.0; #fail\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "## set policy\n",
    "policy = optimalWithNoise_policy\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    while not done:\n",
    "        # s,a,r,s'\n",
    "        a = np.random.choice(actions,p=policy[s,:])\n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        TD_target = reward + gamma * V[s1]\n",
    "        V[s] += alpha*(TD_target-V[s])\n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "        \n",
    "        \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60470404  0.65359558  0.81632922  0.58195965]\n",
      " [ 0.668624    0.71808405  0.87274739  0.73524075]\n",
      " [ 0.74604539  0.82272036  0.93766611  0.6181517 ]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.54817606  0.75965256  0.57324682  0.50477549]\n",
      " [ 0.30544591  0.69986116 -0.36032707  0.19435391]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.42755831  0.69230416  0.39395507  0.35777611]\n",
      " [ 0.62701724  0.30697542  0.29923145  0.33514213]\n",
      " [ 0.56696604  0.25695275  0.11085974  0.13837313]\n",
      " [ 0.36797657 -0.25446626  0.03098838  0.05335194]]\n"
     ]
    }
   ],
   "source": [
    "## TD(0) for Q\n",
    "## set Hyper parameters\n",
    "epoch = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "## set boundary condition\n",
    "Q = np.zeros((N_STATES,N_ACTIONS))\n",
    "Q[3,:] = 1.0; #goal\n",
    "Q[6,:] = -1.0; #fail\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "## set policy\n",
    "policy = optimalWithNoise_policy\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    a = np.random.choice(actions,p=policy[s,:]) # random initial action\n",
    "    while not done:\n",
    "        # s,a,r,s',a'\n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        a1 = np.random.choice(actions,p=policy[s1,:])\n",
    "        TD_target = reward + gamma * Q[s1,a1]\n",
    "        Q[s,a] += alpha*(TD_target-Q[s,a])\n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "        \n",
    "        \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$)\n",
    "#### Forward View(offline)\n",
    "    * Gemetry sumation of goal(Expected cummulative reward)\n",
    "$$G_{t}^{\\lambda}    = (1-\\lambda)\n",
    "    \\sum_{n=1}^{\\infty} \\lambda^{n-1}G_{t}^{(n)}$$\n",
    "$$V(S_{t})\\: \\leftarrow\\:V(S_{t})\\:+\\:\\alpha\\:*\\:(G_{t}^{\\lambda}-V(S_{t}))$$\n",
    "    * it is hard to implement with this form(high complexity and memory uses)\n",
    "#### Backward View(online)\n",
    "    * Eligibility traces\n",
    "$$ E_{t}(s) = \\gamma \\lambda E_{t-1}(s)+1(S=S_{t})$$\n",
    "    \n",
    "    * TD lambda\n",
    "$$V(S) \\leftarrow V(S)+ \\alpha \\delta_{t} E_{t}(S) $$  \n",
    "<center>  </center>\n",
    "<center>where $\\delta_{t} = R_{t+1}\\:+\\:\\gamma V(S_{t+1})-V(S_{t})$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83401321  0.89166407  0.92472041  1.          0.78134167  0.57062885\n",
      " -1.          0.67997237  0.56763827  0.3895605   0.03745909]\n"
     ]
    }
   ],
   "source": [
    "## TD(lamda) for V\n",
    "\n",
    "## set Hyper parameters\n",
    "epoch = 1000\n",
    "alpha = 0.01\n",
    "lam = 0.5\n",
    "\n",
    "## set boundary condition\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1.0; #goal\n",
    "V[6] = -1.0; #fail\n",
    "\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "## set policy\n",
    "policy = optimalWithNoise_policy\n",
    "E_history = np.zeros((100,N_STATES))\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    \n",
    "    #set Eligibilty traces\n",
    "    E = np.zeros(N_STATES)\n",
    "    \n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    t = 1\n",
    "    while not done:\n",
    "        # s,a,r,s'\n",
    "        a = np.random.choice(actions,p=policy[s,:])\n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        \n",
    "\n",
    "        TD_target = reward + gamma * V[s1]\n",
    "        TD_error = TD_target-V[s]\n",
    "        E[s]+=1\n",
    "        \n",
    "        for state in start_states:\n",
    "            V[state] += alpha*TD_error*E[state]\n",
    "            E[state] = gamma*lam*E[state]\n",
    "        E_history[t,:]=E \n",
    "        t +=1\n",
    "    \n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "            \n",
    "        else:\n",
    "            s = s1\n",
    "           \n",
    "        \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-810388178f5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\donghyunsung\\anaconda3\\envs\\rl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2315\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<c:\\users\\donghyunsung\\anaconda3\\envs\\rl\\lib\\site-packages\\decorator.py:decorator-gen-109>\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\donghyunsung\\anaconda3\\envs\\rl\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\donghyunsung\\anaconda3\\envs\\rl\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\donghyunsung\\anaconda3\\envs\\rl\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   3405\u001b[0m         \"\"\"\n\u001b[0;32m   3406\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3407\u001b[1;33m         \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3409\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\donghyunsung\\anaconda3\\envs\\rl\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[1;34m(gui, gui_select)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \"\"\"\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(20, 20)) \n",
    "\n",
    "for i in states:\n",
    "    plt.subplot(4,3,i+1).set_title(\"Eligibility Traces for state {}\".format(str(i)))\n",
    "    plt.plot(E_history[:,i])\n",
    "    plt.legend(['state {}'.format(str(i))])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.42257604  0.50048387  0.80205974  0.41270311]\n",
      " [ 0.52474496  0.57978962  0.87128712  0.54896926]\n",
      " [ 0.60896887  0.68688252  0.90578188  0.46352214]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.37803751  0.75240565  0.42872495  0.38886904]\n",
      " [ 0.15089564  0.63712353 -0.16239544  0.02602443]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.28029129  0.69267805  0.21952781  0.28367253]\n",
      " [ 0.63503676  0.2378022   0.21004956  0.2195161 ]\n",
      " [ 0.56939818  0.17418369  0.07737392  0.11467944]\n",
      " [ 0.3869438  -0.10811413 -0.00533715  0.04231542]]\n"
     ]
    }
   ],
   "source": [
    "## TD(lamda) for Q\n",
    "\n",
    "## set Hyper parameters\n",
    "epoch = 5000\n",
    "alpha = 0.01\n",
    "lam = 0.5\n",
    "\n",
    "## set boundary condition\n",
    "Q = np.zeros((N_STATES,N_ACTIONS))\n",
    "Q[3,:] = 1.0; #goal\n",
    "Q[6,:] = -1.0; #fail\n",
    "\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "## set policy\n",
    "policy = optimalWithNoise_policy\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    \n",
    "    #set Eligibilty traces\n",
    "    E = np.zeros((N_STATES,N_ACTIONS))\n",
    "    \n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    a = np.random.choice(actions,p=policy[s,:]) # random initial action\n",
    "    while not done:\n",
    "        # s,a,r,s',a'\n",
    "        \n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        a1 = np.random.choice(actions,p=policy[s1,:])\n",
    "\n",
    "        TD_target = reward + gamma * Q[s1,a1]\n",
    "        TD_error = TD_target - Q[s,a]\n",
    "        E[s,a]+=1\n",
    "        \n",
    "        for state in start_states:\n",
    "            for action in actions:\n",
    "                Q[state,action] += alpha*TD_error*E[state,action]\n",
    "                E[state,action] = gamma*lam*E[state,action]\n",
    "                \n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "           \n",
    "        \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
