{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code \n",
    "#### Grid World Enviroment setting\n",
    "* states, actions, transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set state\n",
    "import numpy as np\n",
    "nCols = 3\n",
    "nRows = 4\n",
    "nWalls = 1\n",
    "states = []\n",
    "for i in range(nCols*nRows-nWalls):\n",
    "    states.append(i)\n",
    "N_STATES = len(states)\n",
    "#print(N_STATES)\n",
    "#print(states)\n",
    "\n",
    "# set map\n",
    "map = -np.ones((nCols+2,nRows+2))\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        map[i+1,j+1] = 0\n",
    "map[2,2] = -1 # add wall\n",
    "#print(map)\n",
    "\n",
    "# set action\n",
    "actions = [0, 1, 2, 3]\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# states -> location\n",
    "locations = []\n",
    "index = 0\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        if map[i+1,j+1]==0:\n",
    "            locations.append((i+1,j+1))\n",
    "            index = index + 1\n",
    "#print(locations) # match index with states\n",
    "# action -> move\n",
    "move = [(0,-1),(-1,0),(0,1),(1,0)] # match index with actions\n",
    "#print(move)\n",
    "\n",
    "# set transition probability\n",
    "P = np.zeros((N_STATES,N_ACTIONS,N_STATES)) # P[S,A,S']\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        current_location = locations[s]\n",
    "        # heading collectly  ####################################################################################\n",
    "        next_location = (current_location[0] + move[a][0],current_location[1] + move[a][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.8\n",
    "        # left error ############################################################################################\n",
    "        next_location = (current_location[0] + move[a-1][0],current_location[1] + move[a-1][1])\n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        # right error ############################################################################################\n",
    "        next_location = (current_location[0] + move[(a+1)%4][0],current_location[1] + move[(a+1)%4][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        \n",
    "# rewards s,a ---  R(s,a)  ---> s'\n",
    "if True:\n",
    "    R = -0.02*np.ones((N_STATES,N_ACTIONS))\n",
    "else:\n",
    "    R = -0.5*np.ones((N_STATES,N_ACTIONS))\n",
    "R[3,:] = 1\n",
    "R[6,:] = -1\n",
    "#print(R)\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy : given state which action would u choose\n",
    "# assume that we know the policy\n",
    "bad_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "bad_policy[0,2] = 1\n",
    "bad_policy[1,2] = 1\n",
    "bad_policy[2,2] = 1\n",
    "bad_policy[3,2] = 1\n",
    "bad_policy[4,3] = 1\n",
    "bad_policy[5,2] = 1\n",
    "bad_policy[6,2] = 1\n",
    "bad_policy[7,2] = 1\n",
    "bad_policy[8,2] = 1\n",
    "bad_policy[9,2] = 1\n",
    "bad_policy[10,1] = 1\n",
    "\n",
    "random_policy = 0.25*np.ones((N_STATES,N_ACTIONS))\n",
    "\n",
    "optimal_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "optimal_policy[0,2] = 1\n",
    "optimal_policy[1,2] = 1\n",
    "optimal_policy[2,2] = 1\n",
    "optimal_policy[3,2] = 1\n",
    "optimal_policy[4,1] = 1\n",
    "optimal_policy[5,1] = 1\n",
    "optimal_policy[6,1] = 1\n",
    "optimal_policy[7,1] = 1\n",
    "optimal_policy[8,0] = 1\n",
    "optimal_policy[9,0] = 1\n",
    "optimal_policy[10,0] = 1\n",
    "#print(optimal_policy)\n",
    "\n",
    "optimalWithNoise_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "ep = 0.1\n",
    "optimalWithNoise_policy[0,2] = 1\n",
    "optimalWithNoise_policy[1,2] = 1\n",
    "optimalWithNoise_policy[2,2] = 1\n",
    "optimalWithNoise_policy[3,2] = 1\n",
    "optimalWithNoise_policy[4,1] = 1\n",
    "optimalWithNoise_policy[5,1] = 1\n",
    "optimalWithNoise_policy[6,1] = 1\n",
    "optimalWithNoise_policy[7,1] = 1\n",
    "optimalWithNoise_policy[8,0] = 1\n",
    "optimalWithNoise_policy[9,0] = 1\n",
    "optimalWithNoise_policy[10,0] = 1\n",
    "optimalWithNoise_policy = optimalWithNoise_policy + (ep/4)*np.ones((N_STATES,N_ACTIONS))\n",
    "optimalWithNoise_policy = optimalWithNoise_policy / np.sum(optimalWithNoise_policy,axis = 1).reshape((N_STATES,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA and SARSA lambda\n",
    "\n",
    "#### SARSA\n",
    "* It is similar to TD(0)\n",
    "* Main difference is TD(0) uses to evaluate policy,but SARSA evaulates policy and improves policy using epsilon greed method to gurantee continual exploraion\n",
    "$$\\:\\:$$\n",
    "$$Q(s_{t},a_{t})\\: \\leftarrow\\:Q(s_{t},a_{t})\\:+\\:\\alpha\\:*\\:(R_{t+1}\\:+\\:\\gamma Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t}))$$\n",
    "$$\\:\\:$$\n",
    "$$ \\pi(a|s) = \\begin{cases} \n",
    "     \\epsilon /m +1-\\epsilon\\:\\:\\:\\:if \\:\\:a^{*}=arg\\max_{a \\in A}Q(s,a) \\\\\n",
    "     \\epsilon /m \\:\\:\\:\\: otherwise\\\\\n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "#### SARSA lambda\n",
    "\n",
    "* It is similar to TD($\\lambda$)\n",
    "$$Q(s,a) \\leftarrow Q(s,a)+ \\alpha \\delta_{t} E_{t}(s,a) $$  \n",
    "\n",
    "$$\\:\\:$$\n",
    "$$ where,\\:\\: \\delta_{t} = R_{t+1}\\:+\\:\\gamma Q(S_{t+1},A_{t})-Q(S_{t},A_{t})$$\n",
    "$$\\:\\:$$\n",
    "$$ E_{t}(s,a) = \\gamma \\lambda E_{t-1}(s,a)+1(S=S_{t},A=A_{t})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38263631  0.3710673   0.77966261  0.32469697]\n",
      " [ 0.47526795  0.55321764  0.85965791  0.55087806]\n",
      " [ 0.73657767  0.82285458  0.93453962  0.59723866]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.21422818  0.71098731  0.18697073  0.13608655]\n",
      " [ 0.47308883  0.67180262 -0.5867729   0.25589752]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.08376768  0.12610414  0.4340622   0.07886718]\n",
      " [ 0.19510033  0.23732698  0.47069782  0.19716453]\n",
      " [ 0.31521713  0.52385982  0.22850195  0.33963809]\n",
      " [ 0.26934153 -0.30306648  0.00204687  0.08277133]]\n",
      "\n",
      "[[0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "## SARSA\n",
    "\n",
    "def Egreedy(eps,sQ):\n",
    "    prob = np.random.uniform(0,1)\n",
    "    if prob>0.1:\n",
    "        return np.argmax(sQ)\n",
    "    else:\n",
    "        return np.random.choice(actions)\n",
    "    \n",
    "## set Hyper parameters\n",
    "epoch = 10000\n",
    "alpha = 0.01\n",
    "eps = 0.1\n",
    "## set boundary condition\n",
    "Q = np.zeros((N_STATES,N_ACTIONS))\n",
    "Q[3,:] = 1.0; #goal\n",
    "Q[6,:] = -1.0; #fail\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    a = Egreedy(eps,Q[s,:]) # random initial action\n",
    "    while not done:\n",
    "        # s,a,r,s',a'\n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        # choose action followng epslion greedy\n",
    "        a1 = Egreedy(eps,Q[s1,:])\n",
    "        TD_target = reward + gamma * Q[s1,a1]\n",
    "        Q[s,a] += alpha*(TD_target - Q[s,a])\n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "        \n",
    "policy = np.eye(N_ACTIONS)[np.argmax(Q,axis=1)].astype(np.int32)        \n",
    "print(Q)\n",
    "print()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25695284  0.35445463  0.7725692   0.31705707]\n",
      " [ 0.42963697  0.39385691  0.83475783  0.42267553]\n",
      " [ 0.64885198  0.64447305  0.88459019  0.47260521]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.16638398  0.72157964  0.1985347   0.1666512 ]\n",
      " [ 0.44372762  0.63058826 -0.58626212  0.22558002]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.09930968  0.64296816  0.058027    0.05700974]\n",
      " [ 0.08653609  0.05192211  0.48246976  0.11806629]\n",
      " [ 0.20431932  0.55392741  0.09877312  0.17293692]\n",
      " [ 0.29634224 -0.21079455 -0.00245975  0.03839282]]\n",
      "\n",
      "[[0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "## SARSA lambda\n",
    "\n",
    "def Egreedy(eps,sQ):\n",
    "    prob = np.random.uniform(0,1)\n",
    "    if prob>0.1:\n",
    "        return np.argmax(sQ)\n",
    "    else:\n",
    "        return np.random.choice(actions)\n",
    "    \n",
    "## set Hyper parameters\n",
    "epoch = 5000\n",
    "alpha = 0.01\n",
    "lam = 0.5\n",
    "eps = 0.1\n",
    "\n",
    "## set boundary condition\n",
    "Q = np.zeros((N_STATES,N_ACTIONS))\n",
    "Q[3,:] = 1.0; #goal\n",
    "Q[6,:] = -1.0; #fail\n",
    "\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    \n",
    "    #set Eligibilty traces\n",
    "    E = np.zeros((N_STATES,N_ACTIONS))\n",
    "    \n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    a = Egreedy(eps,Q[s,:]) # random initial action\n",
    "    while not done:\n",
    "        # s,a,r,s',a'\n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        a1 = Egreedy(eps,Q[s1,:])\n",
    "\n",
    "        TD_target = reward + gamma * Q[s1,a1]\n",
    "        TD_error = TD_target - Q[s,a]\n",
    "        E[s,a]+=1\n",
    "        \n",
    "        for state in start_states:\n",
    "            for action in actions:\n",
    "                Q[state,action] += alpha*TD_error*E[state,action]\n",
    "                E[state,action] = gamma*lam*E[state,action]\n",
    "                \n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "           \n",
    "        \n",
    "policy = np.eye(N_ACTIONS)[np.argmax(Q,axis=1)].astype(np.int32)      \n",
    "print(Q)\n",
    "print()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "$$Q(s,a)\\: \\leftarrow\\:Q(s,a)\\:+\\:\\alpha\\:*\\:(R_{t+1}\\:+\\:\\gamma \\max_{s'}Q(s',a')-Q(s,a))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21133308  0.23828581  0.84744808  0.21029083]\n",
      " [ 0.30940754  0.37487895  0.8901476   0.32701586]\n",
      " [ 0.62743622  0.69990103  0.92917226  0.50942265]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.14078738  0.81103296  0.12507177  0.09246588]\n",
      " [ 0.41436474  0.63782759 -0.38882892  0.2163459 ]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.04045299  0.08988973  0.5587349   0.06927967]\n",
      " [ 0.13088359  0.15071278  0.58335587  0.14030293]\n",
      " [ 0.22624291  0.61157514  0.14416789  0.22493183]\n",
      " [ 0.46043638 -0.26227035  0.01431345  0.02346715]]\n",
      "\n",
      "[[0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "## Q-learning\n",
    "def Egreedy(eps,sQ):\n",
    "    prob = np.random.uniform(0,1)\n",
    "    if prob>0.1:\n",
    "        return np.argmax(sQ)\n",
    "    else:\n",
    "        return np.random.choice(actions)\n",
    "    \n",
    "## set Hyper parameters\n",
    "epoch = 5000\n",
    "alpha = 0.01\n",
    "lam = 0.5\n",
    "eps = 0.1\n",
    "\n",
    "## set boundary condition\n",
    "Q = np.zeros((N_STATES,N_ACTIONS))\n",
    "Q[3,:] = 1.0; #goal\n",
    "Q[6,:] = -1.0; #fail\n",
    "\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    s = np.random.choice(start_states)\n",
    "    a = Egreedy(eps,Q[s,:])\n",
    "    while not done:\n",
    "        # sa r s'a'\n",
    "        reward = R[s,a]\n",
    "        \n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        a1 = Egreedy(eps,Q[s1,:])\n",
    "        Q[s,a] += alpha*(reward + gamma*np.max(Q[s1,:])-Q[s,a])\n",
    "        if s1 in terminal_states:\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "\n",
    "            \n",
    "print(Q)\n",
    "print()\n",
    "policy = np.eye(N_ACTIONS)[np.argmax(Q,axis=1)].astype(np.int32)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "* Due to small sample size(batch size), 3 algorithms (SARSA,SARSA_lambda,Q-learning) have high variance in their output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
