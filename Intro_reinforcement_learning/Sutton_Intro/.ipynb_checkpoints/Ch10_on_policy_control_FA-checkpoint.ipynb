{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch.10 On-policy Control with Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we use action-value function to control ($\\epsilon$-greedy w.r.t q). All the formulations are similar to previous chapter with subsituting state-value function to action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_{t+1}\\:=w_{t} + α*[R_{t+1} + γq(S_{t+1}, A_{t+1}; w_{t}) − q(S_{t},A_{t}; w_{t})]\\nabla q(S_{t}; A_{t}; w_{t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Average Reward setting for Continuous Tasks\n",
    "\n",
    "In most real world problem, there is no terminal state such as walking, running and living. In this situation, discounted reward setting is equal to average reward setting if you apply small trick.\n",
    "* Recap\n",
    "    * Value Function Definition(following current policy $\\pi$\n",
    "$$v_{\\pi}(s) = E[G_{t}|S_{t} = s] $$\n",
    "$$ $$\n",
    "$$q_{\\pi}(s,a) = E[G_{t}|S_{t} = s,A_{t} = a] $$\n",
    "$$ $$\n",
    "    * Cummulative Discounted Reward(in infinite horizon case)\n",
    "$$G_{t} = \\sum_{k=0}^{\\infty} \\gamma^{k}*R_{t+k+1}$$\n",
    "$$ $$\n",
    "    * Stationary Distribution($\\mu(s)$)\n",
    "$$\\eta(s) = h(s)+\\sum_{\\bar{s}}\\eta(\\bar{s})\\sum_{a}\\pi(a|\\bar{s})p(s|\\bar{s},a) $$\n",
    "$$ $$\n",
    "$$\\mu(s) = \\frac{\\eta(s)}{\\sum_{s}\\eta(s)},\\:\\: for\\: all\\: s \\in S $$\n",
    "$$ $$\n",
    "* Average rate of reward\n",
    "    $$ r(\\pi) = \\lim_{h\\rightarrow\\infty}\\sum_{t=1}^{h} E[R_{t}|A_{0:t-1}\\sim\\pi]$$\n",
    "    * This means that expected reward if you follow policy $\\pi$\n",
    "    * We assume that time goes infinity. This allows us to assume stationary distribution.\n",
    "    * Previous derivation, we see that average rate of reward becomes expected reward. Now, we view problem another way. In stationary distribution, it does not depend on start state. After long time passed, state $s$ transits to all possilbe $s'$ with expected reward $\\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)r $\n",
    "    * Finally, for all state s with stationary distribution($\\mu(s)$)\n",
    "$$ $$\n",
    "$$r(\\pi)\\:=\\:\\sum_{s}\\mu(s)\\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)r  $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
