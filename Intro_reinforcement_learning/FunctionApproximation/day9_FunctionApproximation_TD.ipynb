{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World Enviroment setting\n",
    "* states, actions, transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set state\n",
    "import numpy as np\n",
    "nCols = 3\n",
    "nRows = 4\n",
    "nWalls = 1\n",
    "states = []\n",
    "for i in range(nCols*nRows-nWalls):\n",
    "    states.append(i)\n",
    "N_STATES = len(states)\n",
    "\n",
    "terminal_state = [3,6]\n",
    "win_state = [3]\n",
    "lose_state = [6]\n",
    "start_state = [x for x in states if x not in terminal_state]\n",
    "#print(N_STATES)\n",
    "#print(states)\n",
    "\n",
    "# set map\n",
    "map = -np.ones((nCols+2,nRows+2))\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        map[i+1,j+1] = 0\n",
    "map[2,2] = -1 # add wall\n",
    "#print(map)\n",
    "\n",
    "# set action\n",
    "actions = [0, 1, 2, 3]\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# states -> location\n",
    "locations = []\n",
    "index = 0\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        if map[i+1,j+1]==0:\n",
    "            locations.append((i+1,j+1))\n",
    "            index = index + 1\n",
    "#print(locations) # match index with states\n",
    "# action -> move\n",
    "move = [(0,-1),(-1,0),(0,1),(1,0)] # match index with actions\n",
    "#print(move)\n",
    "\n",
    "# set transition probability\n",
    "P = np.zeros((N_STATES,N_ACTIONS,N_STATES)) # P[S,A,S']\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        current_location = locations[s]\n",
    "        # heading collectly  ####################################################################################\n",
    "        next_location = (current_location[0] + move[a][0],current_location[1] + move[a][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.8\n",
    "        # left error ############################################################################################\n",
    "        next_location = (current_location[0] + move[a-1][0],current_location[1] + move[a-1][1])\n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        # right error ############################################################################################\n",
    "        next_location = (current_location[0] + move[(a+1)%4][0],current_location[1] + move[(a+1)%4][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        \n",
    "# rewards s,a ---  R(s,a)  ---> s'\n",
    "if True:\n",
    "    R = -0.02*np.ones((N_STATES,N_ACTIONS))\n",
    "else:\n",
    "    R = -0.5*np.ones((N_STATES,N_ACTIONS))\n",
    "R[3,:] = 1\n",
    "R[6,:] = -1\n",
    "#print(R)\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy : given state which action would u choose\n",
    "# assume that we know the policy\n",
    "bad_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "bad_policy[0,2] = 1\n",
    "bad_policy[1,2] = 1\n",
    "bad_policy[2,2] = 1\n",
    "bad_policy[3,2] = 1\n",
    "bad_policy[4,3] = 1\n",
    "bad_policy[5,2] = 1\n",
    "bad_policy[6,2] = 1\n",
    "bad_policy[7,2] = 1\n",
    "bad_policy[8,2] = 1\n",
    "bad_policy[9,2] = 1\n",
    "bad_policy[10,1] = 1\n",
    "\n",
    "random_policy = 0.25*np.ones((N_STATES,N_ACTIONS))\n",
    "\n",
    "optimal_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "optimal_policy[0,2] = 1\n",
    "optimal_policy[1,2] = 1\n",
    "optimal_policy[2,2] = 1\n",
    "optimal_policy[3,2] = 1\n",
    "optimal_policy[4,1] = 1\n",
    "optimal_policy[5,1] = 1\n",
    "optimal_policy[6,1] = 1\n",
    "optimal_policy[7,1] = 1\n",
    "optimal_policy[8,0] = 1\n",
    "optimal_policy[9,0] = 1\n",
    "optimal_policy[10,0] = 1\n",
    "#print(optimal_policy)\n",
    "\n",
    "optimalWithNoise_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "ep = 0.1\n",
    "optimalWithNoise_policy[0,2] = 1\n",
    "optimalWithNoise_policy[1,2] = 1\n",
    "optimalWithNoise_policy[2,2] = 1\n",
    "optimalWithNoise_policy[3,2] = 1\n",
    "optimalWithNoise_policy[4,1] = 1\n",
    "optimalWithNoise_policy[5,1] = 1\n",
    "optimalWithNoise_policy[6,1] = 1\n",
    "optimalWithNoise_policy[7,1] = 1\n",
    "optimalWithNoise_policy[8,0] = 1\n",
    "optimalWithNoise_policy[9,0] = 1\n",
    "optimalWithNoise_policy[10,0] = 1\n",
    "optimalWithNoise_policy = optimalWithNoise_policy + (ep/4)*np.ones((N_STATES,N_ACTIONS))\n",
    "optimalWithNoise_policy = optimalWithNoise_policy / np.sum(optimalWithNoise_policy,axis = 1).reshape((N_STATES,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD policy evaluation using function approximation - online\n",
    "* Function Approximation(linear combination)\n",
    "$$V(s) = X(s)^{T}w$$\n",
    "* loss function\n",
    "$$J(w) = E_{\\pi}[(R_{t+1}+\\gamma X(s')^{T}w^{-} \\:-\\:X(s)^{T}w)^{2}]$$\n",
    "* gradient descent\n",
    "$$\\nabla_{w}J(w) = -2\\:*\\:E_{\\pi}[(R_{t+1}+\\gamma X(s')^{T}w^{-}\\:-\\:X(s)^{T}w)]\\:*\\:X(s)$$\n",
    "* stochastic gradient descent(batch size 1)\n",
    "$$\\nabla_{w}J(w) = -2\\:*(R_{t+1}+\\gamma X(s')^{T}w^{-}\\:-\\:X(s)^{T}w)\\:*\\:X(s)$$\n",
    "* update parameter vector w\n",
    "$$\\Delta w = \\alpha\\:*\\:(R_{t+1}+\\gamma X(s')^{T}w^{-}\\:-\\:X(s)^{T}w)*X(s) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "import time\n",
    "start = time.time()\n",
    "print(\"Tensorflow version : \")\n",
    "print(tf.__version__)\n",
    "print()\n",
    "\n",
    "## set HyperParemeters\n",
    "epoch = 10000\n",
    "lr_rate = 0.01\n",
    "copy_period = 1\n",
    "policy = optimalWithNoise_policy # Evaluation -> follow given policy\n",
    "\n",
    "Q_final = np.empty((N_STATES,N_ACTIONS))\n",
    "\n",
    "\n",
    "# s a r s a\n",
    "state_tf = tf.placeholder(tf.int32, shape = [None], name = 'state')\n",
    "action_tf = tf.placeholder(tf.int32, shape = [None], name = 'action')\n",
    "reward_tf = tf.placeholder(tf.float32, shape = [None], name = 'reward')\n",
    "next_state_tf = tf.placeholder(tf.int32, shape = [None], name = 'next_state')\n",
    "next_action_tf = tf.placeholder(tf.int32, shape = [None], name = 'next_action')\n",
    "done_holder = tf.placeholder(tf.bool, shape = [None], name = 'done')\n",
    "with tf.variable_scope('main_net') as scope:\n",
    "    W = tf.get_variable(name='W', \\\n",
    "                        shape=[N_STATES + N_ACTIONS, 1], \\\n",
    "                        dtype=tf.float32, \\\n",
    "                        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "with tf.variable_scope('target_net') as scope:\n",
    "    W_target = tf.get_variable(name='W_target', \\\n",
    "                        shape=[N_STATES + N_ACTIONS, 1], \\\n",
    "                        dtype=tf.float32, \\\n",
    "                        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "state_tf_one_hot = tf.one_hot(state_tf, N_STATES)\n",
    "action_tf_one_hot = tf.one_hot(action_tf, N_ACTIONS)\n",
    "next_state_tf_one_hot = tf.one_hot(next_state_tf, N_STATES)\n",
    "next_action_tf_one_hot = tf.one_hot(next_action_tf, N_ACTIONS)\n",
    "\n",
    "state_action_tf_one_hot = tf.concat([state_tf_one_hot, action_tf_one_hot],1)\n",
    "next_state_action_tf_one_hot = tf.concat([next_state_tf_one_hot, next_action_tf_one_hot],1)\n",
    "\n",
    "\n",
    "from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'main_net')\n",
    "to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'target_net')\n",
    "\n",
    "copy_ops = [to_vars[i].assign(from_vars[i]) for i in range(len(from_vars))]\n",
    "\n",
    "TD_target_not_done = reward_tf + gamma * tf.matmul(next_state_action_tf_one_hot, W_target)\n",
    "TD_target_done = reward_tf # this reward is actually reward + gamma * final_reward\n",
    "\n",
    "    \n",
    "Q = tf.matmul(state_action_tf_one_hot,W) # function approximation\n",
    "\n",
    "\n",
    "TD_error_not_done = TD_target_not_done - Q\n",
    "TD_error_done = TD_target_done - Q    \n",
    "    \n",
    "    \n",
    "error = tf.to_float(done_holder)*TD_error_done + (1-tf.to_float(done_holder))*TD_error_not_done\n",
    "loss = tf.reduce_mean(tf.square(error))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=lr_rate)\n",
    "train_ops = opt.minimize(loss,var_list = from_vars)    \n",
    "    \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run() # initialize parameters following pre-defined recipe\n",
    "    gradient_update_number = 0 \n",
    "    \n",
    "    for _ in range(epoch):\n",
    "        done = False\n",
    "        print(str(_)+\"th iteration\")\n",
    "        s = np.random.choice(start_state) # random initial state\n",
    "        a = np.random.choice(actions,p=policy[s,:])\n",
    "        \n",
    "        while not done:\n",
    "            if gradient_update_number % copy_period == 0:\n",
    "                sess.run(copy_ops) # W target is previous W to make W converge\n",
    "                \n",
    "            r = R[s,a]\n",
    "            s1 = np.random.choice(states,p = P[s,a,:])\n",
    "            a1 = np.random.choice(actions,p = policy[s1,:])\n",
    "            \n",
    "            if s1 in terminal_state:\n",
    "                done = True            \n",
    "                feed_dict = {state_tf: [s], action_tf: [a],reward_tf: [r], next_state_tf: [s1], next_action_tf: [a1], done_holder: [done]}\n",
    "                sess.run(train_ops, feed_dict=feed_dict)\n",
    "                gradient_update_number += 1\n",
    "            \n",
    "            else:\n",
    "                feed_dict = {state_tf: [s], action_tf: [a],reward_tf: [r], next_state_tf: [s1], next_action_tf: [a1], done_holder: [done]}\n",
    "                sess.run(train_ops, feed_dict=feed_dict)\n",
    "                gradient_update_number += 1\n",
    "                s = s1\n",
    "                a = a1\n",
    "\n",
    " \n",
    "\n",
    "    for s in range(N_STATES):\n",
    "        for a in range(N_ACTIONS):\n",
    "            feed_dict = {state_tf: [s], action_tf: [a]}\n",
    "            Q_now = sess.run(Q, feed_dict=feed_dict)\n",
    "            Q_final[s,a] = Q_now[0][0]\n",
    "\n",
    "    \n",
    "print(Q_final)    \n",
    "print()   \n",
    "print(\"it takes \"+str(round(time.time()-start))+\" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "* with TD, it takes long time to converge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
