{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World Enviroment setting\n",
    "* states, actions, transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set state\n",
    "import numpy as np\n",
    "nCols = 3\n",
    "nRows = 4\n",
    "nWalls = 1\n",
    "states = []\n",
    "for i in range(nCols*nRows-nWalls):\n",
    "    states.append(i)\n",
    "N_STATES = len(states)\n",
    "#print(N_STATES)\n",
    "#print(states)\n",
    "\n",
    "# set map\n",
    "map = -np.ones((nCols+2,nRows+2))\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        map[i+1,j+1] = 0\n",
    "map[2,2] = -1 # add wall\n",
    "#print(map)\n",
    "\n",
    "# set action\n",
    "actions = [0, 1, 2, 3]\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# states -> location\n",
    "locations = []\n",
    "index = 0\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        if map[i+1,j+1]==0:\n",
    "            locations.append((i+1,j+1))\n",
    "            index = index + 1\n",
    "#print(locations) # match index with states\n",
    "# action -> move\n",
    "move = [(0,-1),(-1,0),(0,1),(1,0)] # match index with actions\n",
    "#print(move)\n",
    "\n",
    "# set transition probability\n",
    "P = np.zeros((N_STATES,N_ACTIONS,N_STATES)) # P[S,A,S']\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        current_location = locations[s]\n",
    "        # heading collectly  ####################################################################################\n",
    "        next_location = (current_location[0] + move[a][0],current_location[1] + move[a][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.8\n",
    "        # left error ############################################################################################\n",
    "        next_location = (current_location[0] + move[a-1][0],current_location[1] + move[a-1][1])\n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        # right error ############################################################################################\n",
    "        next_location = (current_location[0] + move[(a+1)%4][0],current_location[1] + move[(a+1)%4][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        \n",
    "# rewards s,a ---  R(s,a)  ---> s'\n",
    "if True:\n",
    "    R = -0.02*np.ones((N_STATES,N_ACTIONS))\n",
    "else:\n",
    "    R = -0.5*np.ones((N_STATES,N_ACTIONS))\n",
    "R[3,:] = 1\n",
    "R[6,:] = -1\n",
    "#print(R)\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy : given state which action would u choose\n",
    "# assume that we know the policy\n",
    "bad_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "bad_policy[0,2] = 1\n",
    "bad_policy[1,2] = 1\n",
    "bad_policy[2,2] = 1\n",
    "bad_policy[3,2] = 1\n",
    "bad_policy[4,3] = 1\n",
    "bad_policy[5,2] = 1\n",
    "bad_policy[6,2] = 1\n",
    "bad_policy[7,2] = 1\n",
    "bad_policy[8,2] = 1\n",
    "bad_policy[9,2] = 1\n",
    "bad_policy[10,1] = 1\n",
    "\n",
    "random_policy = 0.25*np.ones((N_STATES,N_ACTIONS))\n",
    "\n",
    "optimal_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "optimal_policy[0,2] = 1\n",
    "optimal_policy[1,2] = 1\n",
    "optimal_policy[2,2] = 1\n",
    "optimal_policy[3,2] = 1\n",
    "optimal_policy[4,1] = 1\n",
    "optimal_policy[5,1] = 1\n",
    "optimal_policy[6,1] = 1\n",
    "optimal_policy[7,1] = 1\n",
    "optimal_policy[8,0] = 1\n",
    "optimal_policy[9,0] = 1\n",
    "optimal_policy[10,0] = 1\n",
    "#print(optimal_policy)\n",
    "\n",
    "optimalWithNoise_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "ep = 0.1\n",
    "optimalWithNoise_policy[0,2] = 1\n",
    "optimalWithNoise_policy[1,2] = 1\n",
    "optimalWithNoise_policy[2,2] = 1\n",
    "optimalWithNoise_policy[3,2] = 1\n",
    "optimalWithNoise_policy[4,1] = 1\n",
    "optimalWithNoise_policy[5,1] = 1\n",
    "optimalWithNoise_policy[6,1] = 1\n",
    "optimalWithNoise_policy[7,1] = 1\n",
    "optimalWithNoise_policy[8,0] = 1\n",
    "optimalWithNoise_policy[9,0] = 1\n",
    "optimalWithNoise_policy[10,0] = 1\n",
    "optimalWithNoise_policy = optimalWithNoise_policy + (ep/4)*np.ones((N_STATES,N_ACTIONS))\n",
    "optimalWithNoise_policy = optimalWithNoise_policy / np.sum(optimalWithNoise_policy,axis = 1).reshape((N_STATES,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Evaluation\n",
    "\n",
    "### First-visit Monte Carlo\n",
    "\n",
    "* Update number of visit $N(s)$ and cummulative goal $S(s)$ for the first time visited state per on simulaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85389115  0.8958656   0.93666976  1.          0.81694087  0.67932254\n",
      " -1.          0.77673358  0.74188086  0.70777203  0.51644668]\n"
     ]
    }
   ],
   "source": [
    "# First visit Monte Carlo Evaluation for state value function V\n",
    "epoch = 1000 #number of simulation\n",
    "\n",
    "terminal_states = [3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "\n",
    "policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "policy = optimal_policy\n",
    "#print(policy)\n",
    "num_visit = np.zeros(N_STATES) # N(s)\n",
    "cum_gain = np.zeros(N_STATES) # S(s)\n",
    "\n",
    "for _ in range(epoch):\n",
    "    Isvisit = np.zeros(N_STATES)\n",
    "    s = np.random.choice(start_states) #randomly choose initial state\n",
    "    Isvisit[s] = 1\n",
    "    done = False\n",
    "    simulation_history = [] # <- (s,a) pair \n",
    "    reward_history = [] # <- reward pair \n",
    "    goal_history = []\n",
    "    while not done:\n",
    "        # s -> a -> reward -> s1\n",
    "        a = np.random.choice(actions,p=policy[s,:]) #follow policy\n",
    "        reward = R[s,a]\n",
    "        simulation_history.append((s,a))\n",
    "        reward_history.append(reward) # state,reward pair\n",
    "        s1 = np.random.choice(states,p=P[s,a,:]) #eviroment\n",
    "        \n",
    "        Isvisit[s1]+=1\n",
    "        \n",
    "        if s1 in terminal_states:\n",
    "            done = True\n",
    "            if s1 == 3: #goal\n",
    "                simulation_history.append((s1,0))\n",
    "                reward_history.append(1.)\n",
    "            else: #fail\n",
    "                simulation_history.append((s1,0))\n",
    "                reward_history.append(-1.)\n",
    "            # evaluate G(t)\n",
    "            for i,r in enumerate(reward_history[::-1]):\n",
    "                # G(t-1) = reward(t) + gamma * G(t)\n",
    "                # if terminal G(T) = r(T)\n",
    "                # To implent, i use reverse ordering\n",
    "                if i==0:\n",
    "                    goal_history.append(r)\n",
    "                else:\n",
    "                    goal_history.append(gamma * goal_history[i-1] + r)\n",
    "            \n",
    "            goal_history = goal_history[::-1]\n",
    "            num_visit = num_visit +(Isvisit.astype(np.bool)).astype(np.float32) #make 0 or 1\n",
    "            # add G(t) to s(t)\n",
    "            flag = Isvisit.astype(np.bool)\n",
    "            for i,(s,a) in enumerate(simulation_history):\n",
    "                # i for find G(t)\n",
    "                # S(s) = S(s) + G(t) for only first visit.\n",
    "                if flag[s]==True: #mutiple visit during one simulation\n",
    "                    cum_gain[s]= cum_gain[s] + goal_history[i]\n",
    "                    flag[s] = False\n",
    "                \n",
    "                    \n",
    "        else:\n",
    "            s = s1\n",
    "            \n",
    "V = np.zeros(N_STATES)\n",
    "V = cum_gain/(num_visit+1.0e-8)\n",
    "\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55148925 -0.46936557 -0.3441524  -0.59884203]\n",
      " [-0.56936533 -0.31844596 -0.02187794 -0.29040013]\n",
      " [-0.28738026 -0.01601685  0.73034602 -0.47601084]\n",
      " [ 1.          0.          0.          0.        ]\n",
      " [-0.68975223 -0.53047233 -0.67468711 -0.75576919]\n",
      " [-0.65813619 -0.27385222 -0.89633605 -0.85537669]\n",
      " [-1.          0.          0.          0.        ]\n",
      " [-0.78017974 -0.69770819 -0.77754919 -0.80218689]\n",
      " [-0.74186266 -0.82148816 -0.87692793 -0.83933743]\n",
      " [-0.82905135 -0.74825603 -0.90606611 -0.87317067]\n",
      " [-0.87890593 -0.97858248 -0.95103239 -0.96029396]]\n"
     ]
    }
   ],
   "source": [
    "# First visit Monte Carlo Evaluation for state value function Q\n",
    "epoch = 1000 #number of simulation\n",
    "\n",
    "terminal_states = [3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "\n",
    "policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "policy = random_policy\n",
    "#print(policy)\n",
    "num_visit = np.zeros((N_STATES,N_ACTIONS)) # N(s)\n",
    "cum_gain = np.zeros((N_STATES,N_ACTIONS)) # S(s)\n",
    "\n",
    "for _ in range(epoch):\n",
    "    Isvisit = np.zeros((N_STATES,N_ACTIONS))\n",
    "    \n",
    "    s = np.random.choice(start_states) #randomly choose initial state\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    simulation_history = [] # <- (s,a) pair \n",
    "    reward_history = [] # <- reward pair \n",
    "    goal_history = []\n",
    "    \n",
    "    while not done:\n",
    "        # s -> a -> reward -> s1\n",
    "        a = np.random.choice(actions,p=policy[s,:]) #follow policy\n",
    "        reward = R[s,a]\n",
    "        simulation_history.append((s,a))\n",
    "        reward_history.append(reward) # state,reward pair\n",
    "        s1 = np.random.choice(states,p=P[s,a,:]) #eviroment\n",
    "      \n",
    "        if s1 in terminal_states:\n",
    "            done = True\n",
    "            if s1 == 3: #goal\n",
    "                simulation_history.append((s1,0))\n",
    "                reward_history.append(1.)\n",
    "            else: #fail\n",
    "                simulation_history.append((s1,0))\n",
    "                reward_history.append(-1.)             \n",
    "        else:\n",
    "            s = s1\n",
    "            \n",
    "    # evaluate G(t)\n",
    "    for i,r in enumerate(reward_history[::-1]):\n",
    "        # G(t-1) = reward(t) + gamma * G(t)\n",
    "        # if terminal G(T) = r(T)\n",
    "        # To implent, i use reverse ordering\n",
    "        if i==0:\n",
    "            goal_history.append(r)\n",
    "        else:\n",
    "            goal_history.append(gamma * goal_history[i-1] + r)\n",
    "\n",
    "    goal_history = goal_history[::-1]\n",
    "    for i,(s,a) in enumerate(simulation_history):\n",
    "        Isvisit[s,a]+=1\n",
    "    num_visit = num_visit +(Isvisit.astype(np.bool)).astype(np.float32) #make 0 or 1\n",
    "    # add G(t) to s(t)\n",
    "    flag = Isvisit.astype(np.bool)\n",
    "    for i,(s,a) in enumerate(simulation_history):\n",
    "        # i for find G(t)\n",
    "        # S(s) = S(s) + G(t) for only first visit.\n",
    "        if flag[s,a]==True: #mutiple visit during one simulation\n",
    "            cum_gain[s,a]= cum_gain[s,a] + goal_history[i]\n",
    "            flag[s,a] = False\n",
    "            \n",
    "\n",
    "Q = cum_gain/(num_visit+1.0e-8)\n",
    "\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every-visit Monte Carlo\n",
    "\n",
    "* Update number of visit $N(s)$ and cummulative goal $S(s)$ for Every visited state per on simulaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8256328   0.87578043  0.91702693  1.          0.79355776  0.65867051\n",
      " -1.          0.75796815  0.71331364  0.68962608  0.52881131]\n"
     ]
    }
   ],
   "source": [
    "# First visit Monte Carlo Evaluation for state value function V\n",
    "epoch = 1000 #number of simulation\n",
    "\n",
    "terminal_states = [3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "\n",
    "policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "policy = optimal_policy\n",
    "#print(policy)\n",
    "num_visit = np.zeros(N_STATES) # N(s)\n",
    "cum_gain = np.zeros(N_STATES) # S(s)\n",
    "\n",
    "for _ in range(epoch):\n",
    "    Isvisit = np.zeros(N_STATES)\n",
    "    s = np.random.choice(start_states) #randomly choose initial state\n",
    "    Isvisit[s] = 1\n",
    "    done = False\n",
    "    simulation_history = [] # <- (s,a) pair \n",
    "    reward_history = [] # <- reward pair \n",
    "    goal_history = []\n",
    "    while not done:\n",
    "        # s -> a -> reward -> s1\n",
    "        a = np.random.choice(actions,p=policy[s,:]) #follow policy\n",
    "        reward = R[s,a]\n",
    "        simulation_history.append((s,a))\n",
    "        reward_history.append(reward) # state,reward pair\n",
    "        s1 = np.random.choice(states,p=P[s,a,:]) #eviroment\n",
    "        \n",
    "        Isvisit[s1]+=1\n",
    "        \n",
    "        if s1 in terminal_states:\n",
    "            done = True\n",
    "            if s1 == 3: #goal\n",
    "                simulation_history.append((s1,0))\n",
    "                reward_history.append(1.)\n",
    "            else: #fail\n",
    "                simulation_history.append((s1,0))\n",
    "                reward_history.append(-1.)\n",
    "            # evaluate G(t)\n",
    "            for i,r in enumerate(reward_history[::-1]):\n",
    "                # G(t-1) = reward(t) + gamma * G(t)\n",
    "                # if terminal G(T) = r(T)\n",
    "                # To implent, i use reverse ordering\n",
    "                if i==0:\n",
    "                    goal_history.append(r)\n",
    "                else:\n",
    "                    goal_history.append(gamma * goal_history[i-1] + r)\n",
    "            \n",
    "            goal_history = goal_history[::-1]\n",
    "            num_visit = num_visit +Isvisit\n",
    "            # add G(t) to s(t)\n",
    "            flag = Isvisit.astype(np.bool)\n",
    "            for i,(s,a) in enumerate(simulation_history):\n",
    "                # i for find G(t)\n",
    "                # S(s) = S(s) + G(t) for only first visit.\n",
    "                cum_gain[s]= cum_gain[s] + goal_history[i]\n",
    "                \n",
    "                    \n",
    "        else:\n",
    "            s = s1\n",
    "            \n",
    "V = np.zeros(N_STATES)\n",
    "V = cum_gain/(num_visit+1.0e-8)\n",
    "\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.57078387 -0.54416554 -0.31750244 -0.63337734]\n",
      " [-0.44917479 -0.30595862 -0.05940585 -0.2675822 ]\n",
      " [-0.28817657  0.11477241  0.71114004 -0.45494647]\n",
      " [ 1.          0.          0.          0.        ]\n",
      " [-0.64800837 -0.556055   -0.68845086 -0.77438867]\n",
      " [-0.63729069 -0.22859301 -0.84188469 -0.80309987]\n",
      " [-1.          0.          0.          0.        ]\n",
      " [-0.72019628 -0.69785886 -0.80867707 -0.73977236]\n",
      " [-0.7948872  -0.83639431 -0.88547534 -0.86201839]\n",
      " [-0.83802797 -0.76753522 -0.9183512  -0.87134532]\n",
      " [-0.88296322 -0.98646124 -0.96643855 -0.93748471]]\n"
     ]
    }
   ],
   "source": [
    "# First visit Monte Carlo Evaluation for state value function Q\n",
    "epoch = 1000 #number of simulation\n",
    "\n",
    "terminal_states = [3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "\n",
    "policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "policy = random_policy\n",
    "#print(policy)\n",
    "num_visit = np.zeros((N_STATES,N_ACTIONS)) # N(s)\n",
    "cum_gain = np.zeros((N_STATES,N_ACTIONS)) # S(s)\n",
    "\n",
    "for _ in range(epoch):\n",
    "    Isvisit = np.zeros((N_STATES,N_ACTIONS))\n",
    "    \n",
    "    s = np.random.choice(start_states) #randomly choose initial state\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    simulation_history = [] # <- (s,a) pair \n",
    "    reward_history = [] # <- reward pair \n",
    "    goal_history = []\n",
    "    \n",
    "    while not done:\n",
    "        # s -> a -> reward -> s1\n",
    "        a = np.random.choice(actions,p=policy[s,:]) #follow policy\n",
    "        reward = R[s,a]\n",
    "        simulation_history.append((s,a))\n",
    "        reward_history.append(reward) # state,reward pair\n",
    "        s1 = np.random.choice(states,p=P[s,a,:]) #eviroment\n",
    "      \n",
    "        if s1 in terminal_states:\n",
    "            done = True\n",
    "            if s1 == 3: #goal\n",
    "                simulation_history.append((s1,0))\n",
    "                reward_history.append(1.)\n",
    "            else: #fail\n",
    "                simulation_history.append((s1,0))\n",
    "                reward_history.append(-1.)             \n",
    "        else:\n",
    "            s = s1\n",
    "            \n",
    "    # evaluate G(t)\n",
    "    for i,r in enumerate(reward_history[::-1]):\n",
    "        # G(t-1) = reward(t) + gamma * G(t)\n",
    "        # if terminal G(T) = r(T)\n",
    "        # To implent, i use reverse ordering\n",
    "        if i==0:\n",
    "            goal_history.append(r)\n",
    "        else:\n",
    "            goal_history.append(gamma * goal_history[i-1] + r)\n",
    "\n",
    "    goal_history = goal_history[::-1]\n",
    "    for i,(s,a) in enumerate(simulation_history):\n",
    "        Isvisit[s,a]+=1\n",
    "    num_visit = num_visit + Isvisit\n",
    "    # add G(t) to s(t)\n",
    "    for i,(s,a) in enumerate(simulation_history):\n",
    "        # i for find G(t)\n",
    "        # S(s) = S(s) + G(t) for only first visit.\n",
    "        cum_gain[s,a]= cum_gain[s,a] + goal_history[i]\n",
    "            \n",
    "\n",
    "Q = cum_gain/(num_visit+1.0e-8)\n",
    "\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "* If you have determinisitic policy, it is hard to evaluate policy with Q. This is because it always chooses same action when it is in that state.  \n",
    "  \n",
    "$$Expolitation\\rightarrow No Exploration$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
