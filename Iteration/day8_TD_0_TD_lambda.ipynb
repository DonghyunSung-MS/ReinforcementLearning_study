{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Differance ( TD(0) )\n",
    "#### Incremental MC\n",
    "    * Reccurence formula of average calculation\n",
    "$$\\mu_{k} = \\mu_{k-1}+{{1}\\over{k}}(x_{k}-\\mu_{k-1}) $$\n",
    "    * MC(stationary)\n",
    "$$ V(S_{t})\\: \\leftarrow\\:V(S_{t})\\:+\\:{1\\over{N(S)}}\\:*\\:(G_{t}-V(S_{t}))$$\n",
    "    * MC(non-stationary)\n",
    "$$V(S_{t})\\: \\leftarrow\\:V(S_{t})\\:+\\:\\alpha\\:*\\:(G_{t}-V(S_{t}))$$\n",
    "#### TD(0)\n",
    "    * Instead of waiting utill simulation finishes, update value function step by step\n",
    "\n",
    "$$V(S_{t})\\: \\leftarrow\\:V(S_{t})\\:+\\:\\alpha\\:*\\:(R_{t+1}\\:+\\:\\gamma V(S_{t+1})-V(S_{t}))$$\n",
    "    * TD target is bias estimation of real $G_{t}$ and has low variance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code \n",
    "#### Grid World Enviroment setting\n",
    "* states, actions, transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set state\n",
    "import numpy as np\n",
    "nCols = 3\n",
    "nRows = 4\n",
    "nWalls = 1\n",
    "states = []\n",
    "for i in range(nCols*nRows-nWalls):\n",
    "    states.append(i)\n",
    "N_STATES = len(states)\n",
    "#print(N_STATES)\n",
    "#print(states)\n",
    "\n",
    "# set map\n",
    "map = -np.ones((nCols+2,nRows+2))\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        map[i+1,j+1] = 0\n",
    "map[2,2] = -1 # add wall\n",
    "#print(map)\n",
    "\n",
    "# set action\n",
    "actions = [0, 1, 2, 3]\n",
    "N_ACTIONS = len(actions)\n",
    "\n",
    "# states -> location\n",
    "locations = []\n",
    "index = 0\n",
    "for i in range(nCols):\n",
    "    for j in range(nRows):\n",
    "        if map[i+1,j+1]==0:\n",
    "            locations.append((i+1,j+1))\n",
    "            index = index + 1\n",
    "#print(locations) # match index with states\n",
    "# action -> move\n",
    "move = [(0,-1),(-1,0),(0,1),(1,0)] # match index with actions\n",
    "#print(move)\n",
    "\n",
    "# set transition probability\n",
    "P = np.zeros((N_STATES,N_ACTIONS,N_STATES)) # P[S,A,S']\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        current_location = locations[s]\n",
    "        # heading collectly  ####################################################################################\n",
    "        next_location = (current_location[0] + move[a][0],current_location[1] + move[a][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.8\n",
    "        # left error ############################################################################################\n",
    "        next_location = (current_location[0] + move[a-1][0],current_location[1] + move[a-1][1])\n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        # right error ############################################################################################\n",
    "        next_location = (current_location[0] + move[(a+1)%4][0],current_location[1] + move[(a+1)%4][1])\n",
    "        \n",
    "        if map[next_location[0],next_location[1]] == -1: # there is barrier or wall\n",
    "            next_location = current_location\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        else:\n",
    "            next_s = states[locations.index(next_location)]\n",
    "        P[s,a,next_s] = P[s,a,next_s] + 0.1\n",
    "        \n",
    "# rewards s,a ---  R(s,a)  ---> s'\n",
    "if True:\n",
    "    R = -0.02*np.ones((N_STATES,N_ACTIONS))\n",
    "else:\n",
    "    R = -0.5*np.ones((N_STATES,N_ACTIONS))\n",
    "R[3,:] = 1\n",
    "R[6,:] = -1\n",
    "#print(R)\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# policy : given state which action would u choose\n",
    "# assume that we know the policy\n",
    "bad_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "bad_policy[0,2] = 1\n",
    "bad_policy[1,2] = 1\n",
    "bad_policy[2,2] = 1\n",
    "bad_policy[3,2] = 1\n",
    "bad_policy[4,3] = 1\n",
    "bad_policy[5,2] = 1\n",
    "bad_policy[6,2] = 1\n",
    "bad_policy[7,2] = 1\n",
    "bad_policy[8,2] = 1\n",
    "bad_policy[9,2] = 1\n",
    "bad_policy[10,1] = 1\n",
    "\n",
    "random_policy = 0.25*np.ones((N_STATES,N_ACTIONS))\n",
    "\n",
    "optimal_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "optimal_policy[0,2] = 1\n",
    "optimal_policy[1,2] = 1\n",
    "optimal_policy[2,2] = 1\n",
    "optimal_policy[3,2] = 1\n",
    "optimal_policy[4,1] = 1\n",
    "optimal_policy[5,1] = 1\n",
    "optimal_policy[6,1] = 1\n",
    "optimal_policy[7,1] = 1\n",
    "optimal_policy[8,0] = 1\n",
    "optimal_policy[9,0] = 1\n",
    "optimal_policy[10,0] = 1\n",
    "#print(optimal_policy)\n",
    "\n",
    "optimalWithNoise_policy = np.zeros((N_STATES,N_ACTIONS))\n",
    "ep = 0.1\n",
    "optimalWithNoise_policy[0,2] = 1\n",
    "optimalWithNoise_policy[1,2] = 1\n",
    "optimalWithNoise_policy[2,2] = 1\n",
    "optimalWithNoise_policy[3,2] = 1\n",
    "optimalWithNoise_policy[4,1] = 1\n",
    "optimalWithNoise_policy[5,1] = 1\n",
    "optimalWithNoise_policy[6,1] = 1\n",
    "optimalWithNoise_policy[7,1] = 1\n",
    "optimalWithNoise_policy[8,0] = 1\n",
    "optimalWithNoise_policy[9,0] = 1\n",
    "optimalWithNoise_policy[10,0] = 1\n",
    "optimalWithNoise_policy = optimalWithNoise_policy + (ep/4)*np.ones((N_STATES,N_ACTIONS))\n",
    "optimalWithNoise_policy = optimalWithNoise_policy / np.sum(optimalWithNoise_policy,axis = 1).reshape((N_STATES,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84046699  0.88441946  0.93015651  1.          0.80097982  0.6286222\n",
      " -1.          0.75563874  0.7113231   0.66695245  0.39903267]\n"
     ]
    }
   ],
   "source": [
    "## TD(0) for V\n",
    "\n",
    "## set Hyper parameters\n",
    "epoch = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "## set boundary condition\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1.0; #goal\n",
    "V[6] = -1.0; #fail\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "## set policy\n",
    "policy = optimalWithNoise_policy\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    while not done:\n",
    "        # s,a,r,s'\n",
    "        a = np.random.choice(actions,p=policy[s,:])\n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        TD_target = reward + gamma * V[s1]\n",
    "        V[s] += alpha*(TD_target-V[s])\n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "        \n",
    "        \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61745194  0.64023545  0.81453473  0.58264851]\n",
      " [ 0.66269388  0.73014471  0.88158222  0.7234077 ]\n",
      " [ 0.74498349  0.84901116  0.93133556  0.59478868]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.5576531   0.76069636  0.56760348  0.49817596]\n",
      " [ 0.33269092  0.72889241 -0.38937343  0.14097668]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.38444297  0.69974505  0.38594563  0.40759923]\n",
      " [ 0.62660055  0.33330352  0.28276478  0.31925542]\n",
      " [ 0.58313233  0.19011686  0.11862784  0.15427725]\n",
      " [ 0.30141308 -0.26773705  0.02977535  0.06046637]]\n"
     ]
    }
   ],
   "source": [
    "## TD(0) for Q\n",
    "## set Hyper parameters\n",
    "epoch = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "## set boundary condition\n",
    "Q = np.zeros((N_STATES,N_ACTIONS))\n",
    "Q[3,:] = 1.0; #goal\n",
    "Q[6,:] = -1.0; #fail\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "## set policy\n",
    "policy = optimalWithNoise_policy\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    a = np.random.choice(actions,p=policy[s,:]) # random initial action\n",
    "    while not done:\n",
    "        # s,a,r,s',a'\n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        a1 = np.random.choice(actions,p=policy[s1,:])\n",
    "        TD_target = reward + gamma * Q[s1,a1]\n",
    "        Q[s,a] += alpha*(TD_target-Q[s,a])\n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "        \n",
    "        \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$)\n",
    "#### Forward View\n",
    "    * Gemetry sumation of goal(Expected cummulative reward)\n",
    "$$G_{t}^{\\lambda}    = (1-\\lambda)\n",
    "    \\sum_{n=1}^{\\infty} \\lambda^{n-1}G_{t}^{(n)}$$\n",
    "$$V(S_{t})\\: \\leftarrow\\:V(S_{t})\\:+\\:\\alpha\\:*\\:(G_{t}^{\\lambda}-V(S_{t}))$$\n",
    "    * it is hard to implement with this form(high complexity and memory uses)\n",
    "#### Backward View\n",
    "    * Eligibility traces\n",
    "$$ E_{t}(s) = \\gamma \\lambda E_{t-1}(s)+1(S=S_{t})$$\n",
    "    \n",
    "    * TD lambda\n",
    "$$V(S) \\leftarrow V(S)+ \\alpha \\delta_{t} E_{t}(S) $$  \n",
    "<center>  </center>\n",
    "<center>where $\\delta_{t} = R_{t+1}\\:+\\:\\gamma V(S_{t+1})-V(S_{t})$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80806461  0.86000644  0.89496336  1.          0.75962012  0.51971883\n",
      " -1.          0.67885783  0.60194239  0.45935015  0.15968995]\n"
     ]
    }
   ],
   "source": [
    "## TD(lamda) for V\n",
    "\n",
    "## set Hyper parameters\n",
    "epoch = 1000\n",
    "alpha = 0.01\n",
    "lam = 0.5\n",
    "\n",
    "## set boundary condition\n",
    "V = np.zeros(N_STATES)\n",
    "V[3] = 1.0; #goal\n",
    "V[6] = -1.0; #fail\n",
    "\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "## set policy\n",
    "policy = optimalWithNoise_policy\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    \n",
    "    #set Eligibilty traces\n",
    "    E = np.zeros(N_STATES)\n",
    "    \n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    while not done:\n",
    "        # s,a,r,s'\n",
    "        a = np.random.choice(actions,p=policy[s,:])\n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        \n",
    "\n",
    "        TD_target = reward + gamma * V[s1]\n",
    "        TD_error = TD_target-V[s]\n",
    "        E[s]+=1\n",
    "        \n",
    "        for state in start_states:\n",
    "            V[state] += alpha*TD_error*E[state]\n",
    "            E[state] = gamma*lam*E[state]\n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "           \n",
    "        \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51189967  0.44066408  0.79714176  0.40361248]\n",
      " [ 0.50289966  0.51636085  0.85447292  0.57671915]\n",
      " [ 0.60106147  0.67359323  0.91931738  0.47076323]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.40873809  0.74319264  0.39912164  0.33515651]\n",
      " [ 0.1997753   0.58328708 -0.21326994  0.10110476]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.30502827  0.69014794  0.23696462  0.28436913]\n",
      " [ 0.62565197  0.20839876  0.17910879  0.24591319]\n",
      " [ 0.56844719  0.16295257  0.05496183  0.11057138]\n",
      " [ 0.32175622 -0.08734758  0.02597747  0.02059574]]\n"
     ]
    }
   ],
   "source": [
    "## TD(lamda) for V\n",
    "\n",
    "## set Hyper parameters\n",
    "epoch = 5000\n",
    "alpha = 0.01\n",
    "lam = 0.5\n",
    "\n",
    "## set boundary condition\n",
    "Q = np.zeros((N_STATES,N_ACTIONS))\n",
    "Q[3,:] = 1.0; #goal\n",
    "Q[6,:] = -1.0; #fail\n",
    "\n",
    "## states\n",
    "terminal_states =[3,6]\n",
    "start_states = [x for x in states if x not in terminal_states]\n",
    "## set policy\n",
    "policy = optimalWithNoise_policy\n",
    "\n",
    "for _ in range(epoch):\n",
    "    done = False\n",
    "    \n",
    "    #set Eligibilty traces\n",
    "    E = np.zeros((N_STATES,N_ACTIONS))\n",
    "    \n",
    "    s = np.random.choice(start_states) # random initial state\n",
    "    a = np.random.choice(actions,p=policy[s,:]) # random initial action\n",
    "    while not done:\n",
    "        # s,a,r,s',a'\n",
    "        \n",
    "        reward = R[s,a]\n",
    "        s1 = np.random.choice(states,p=P[s,a,:])\n",
    "        a1 = np.random.choice(actions,p=policy[s1,:])\n",
    "\n",
    "        TD_target = reward + gamma * Q[s1,a1]\n",
    "        TD_error = TD_target - Q[s,a]\n",
    "        E[s,a]+=1\n",
    "        \n",
    "        for state in start_states:\n",
    "            for action in actions:\n",
    "                Q[state,action] += alpha*TD_error*E[state,action]\n",
    "                E[state,action] = gamma*lam*E[state,action]\n",
    "                \n",
    "        if (s1==3) or (s1==6):\n",
    "            done = True\n",
    "        else:\n",
    "            s = s1\n",
    "            a = a1\n",
    "           \n",
    "        \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
