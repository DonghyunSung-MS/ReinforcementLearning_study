{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING\n",
    "\n",
    "#### Ref: [literature](https://arxiv.org/abs/1509.02971), [video](https://www.youtube.com/watch?v=tJBIqkC1wWM&feature=youtu.be)\n",
    "## 1. Background Knowledge  \n",
    "### Function Approximation\n",
    "* Neural Network can be used to approximate continuous function\n",
    "[Wiki](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "* It is hard to use look-up table when state and action space have high-demension such as continuous space(real world)\n",
    "\n",
    "### Deep Q Network([Atari2013](https://arxiv.org/abs/1312.5602))\n",
    "#### Key Idea\n",
    "* Use parameterized value function($\\:Q_{\\theta^{Q}}(s,a)$ or $V_{\\theta^{V}}(s)\\:$) with Neural Network\n",
    "* Epsilon greedy action w.r.t  $\\:argmax_{a}Q(s_{t},a)$\n",
    "* Minimize MSE by SGD like supervised learning\n",
    "$$J = ((r + \\gamma\\max_{a'} Q_{\\theta^{Q}}(s',a'))-Q_{\\theta^{Q}}(s,a))^2 $$\n",
    "* Experience replay use replay buffer $D$ to store $s,\\: a,\\: r,\\: s'$ and random sample from buffer to calculate MSE. This method can break the correlationship between samples(like we assume i.i.d in supervised learning)\n",
    "* Fixed Q target is used to make problem non-stationary. It means when updating parameters in action-value function($\\:\\theta^{Q}(s,a)$), target $r + \\gamma\\max_{a'} Q_{\\theta^{Q}}(s',a')$ uses prior parameters instead of new one.\n",
    "\n",
    "#### Architecture\n",
    "* Input(preprocessed image) : gray scale(spatial) + 4 frame(Temporal) ($84\\times84\\times4$)\n",
    "* output1(conv1+relu) : $8\\times8$ 16 filters $\\rightarrow$ (20,20,16)\n",
    "* output2(conv2+relu) : $4\\times4$ 32 filters $\\rightarrow$ (9,9,32)\n",
    "* output3(fully connected) : (256,1\n",
    "* final output(fully connected) : (4~18,1)\n",
    "    \n",
    "#### Limitation\n",
    "* It can handle only discrete and low_dimensional action-space\n",
    "\n",
    "### Actor-Critic\n",
    "* Actor: policy ($\\:\\pi(a|s)$)\n",
    "* Critic: action-value function($\\:Q(s,a)$)\n",
    "\n",
    "### Deep deterministic policy gradient(DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
