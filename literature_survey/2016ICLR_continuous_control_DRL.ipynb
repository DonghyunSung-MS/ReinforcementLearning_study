{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING\n",
    "\n",
    "#### Ref: [literature](https://arxiv.org/abs/1509.02971), [video](https://www.youtube.com/watch?v=tJBIqkC1wWM&feature=youtu.be)\n",
    "## 1. Background Knowledge\n",
    "### 1-1. Various Gradient Descent Method\n",
    "#### Stochastic Gradient Descent(SGD) & Mini-batch Gradient Descent(MSGD)\n",
    "* SGD : Expectation(GD) -> sample one data\n",
    "* MSGD : Expectation(GD) -> sample multiple data(mini-bathc) and average it\n",
    "$$W\\: \\leftarrow\\: W - \\eta\\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "#### Momentum\n",
    "* Move more toward gradient direction\n",
    "$$v\\: \\leftarrow\\: \\alpha v - \\eta\\frac{\\partial L}{\\partial W} $$\n",
    "$$ $$\n",
    "$$W\\: \\leftarrow\\: W +v$$\n",
    "\n",
    "#### Adaptive Gradient Descent(AdaGrad)\n",
    "* The more prameter moves, The slowe it updates\n",
    "$$h\\: \\leftarrow\\: h + \\frac{\\partial L}{\\partial W}\\odot\\frac{\\partial L}{\\partial W} $$\n",
    "$$ $$\n",
    "$$W\\: \\leftarrow\\: W - \\eta{{1}\\over{\\sqrt{h}}}\\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "#### Root Mean Square Propagation(RMSProp)**Atari\n",
    "* To overcome AdaGrad's slower updating\n",
    "$$h\\: \\leftarrow\\: \\rho h + (1-\\rho)\\frac{\\partial L}{\\partial W}\\odot\\frac{\\partial L}{\\partial W} $$\n",
    "$$ $$\n",
    "$$W\\: \\leftarrow\\: W - \\eta{{1}\\over{\\sqrt{h}}}\\frac{\\partial L}{\\partial W}$$\n",
    "  \n",
    "#### Adaptive Gradient Descent With Momentum(Adam)[ref](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)\n",
    "* First Momentum -> average\n",
    "* Second Momentum -> uncentered variance\n",
    "* Use above with moving average.\n",
    "$$m_{t} = \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\:\\:\\:\\:v_{t} = \\beta_2 v_{t-1} + (1-\\beta_2)g_{t}^{2}$$\n",
    "$$ $$\n",
    "* Bias Correction.\n",
    "$$\\hat{m}_{t} = \\frac{m_{t}}{1-\\beta_1^{t}} \\:\\:\\:\\: \\hat{v}_{t} = \\frac{v_{t}}{1-\\beta_2^{t}}$$\n",
    "$$ $$\n",
    "$$W_{t}\\: =\\: W_{t-1} - \\eta\\frac{\\hat{m}_{t}}{\\sqrt{\\hat{v}_{t}+\\epsilon}}$$\n",
    "$$ $$\n",
    "![img](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile22.uf.tistory.com%2Fimage%2F99E889415D15C0E61FD071)\n",
    "\n",
    "### 1-2. Function Approximation\n",
    "* Neural Network can be used to approximate continuous function\n",
    "[Wiki](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "* It is hard to use look-up table when state and action space have high-demension such as continuous space(real world)\n",
    "\n",
    "### 1-3. Deep Q Network([Atari2013](https://arxiv.org/abs/1312.5602))\n",
    "#### Key Idea\n",
    "* Use parameterized value function($\\:Q_{\\theta^{Q}}(s,a)$ or $V_{\\theta^{V}}(s)\\:$) with Neural Network\n",
    "* Epsilon greedy action w.r.t  $\\:argmax_{a}Q(s_{t},a)$\n",
    "* Minimize MSE by SGD like supervised learning\n",
    "$$J = ((r + \\gamma\\max_{a'} Q_{\\theta^{Q}}(s',a'))-Q_{\\theta^{Q}}(s,a))^2 $$\n",
    "* Experience replay use replay buffer $D$ to store $s,\\: a,\\: r,\\: s'$ and random sample from buffer to calculate MSE. This method can break the correlationship between samples(like we assume i.i.d in supervised learning)\n",
    "* Fixed Q target is used to make problem non-stationary. It means when updating parameters in action-value function($\\:\\theta^{Q}(s,a)$), target $r + \\gamma\\max_{a'} Q_{\\theta^{Q}}(s',a')$ uses prior parameters instead of new one.\n",
    "\n",
    "#### Architecture\n",
    "* Input(preprocessed image) : gray scale(spatial) + 4 frame(Temporal) ($84\\times84\\times4$)\n",
    "* output1(conv1+relu) : $8\\times8$ 16 filters $\\rightarrow$ (20,20,16)\n",
    "* output2(conv2+relu) : $4\\times4$ 32 filters $\\rightarrow$ (9,9,32)\n",
    "* output3(fully connected) : (256,1)\n",
    "* final output(fully connected) : (4~18,1)\n",
    "    \n",
    "#### Limitation\n",
    "* It can handle only discrete and low_dimensional action-space\n",
    "\n",
    "### 1-4. Actor-Critic\n",
    "* Actor: \n",
    "    * policy ($\\:\\pi(a|s)$)\n",
    "    * Maximize $Q_{\\pi}(s,a)$\n",
    "* Critic: \n",
    "    * action-value function($\\:Q(s,a)$)\n",
    "    * Minimize mean square bellman error\n",
    "$$J(\\theta) = \\sum_{(s,a,r,s',d)B}(back up - Q)^2$$\n",
    "\n",
    "## 2. Deep deterministic policy gradient(DDPG)\n",
    "### Key Point\n",
    "* AC\n",
    "* action <-- deterministic Policy + gaussion noise **\n",
    "* replay buffer **\n",
    "* polyak updating target Q network **\n",
    "* off-policy\n",
    "* batch normalization\n",
    "* (pixels + low_demensional)raw_sensory input as state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
