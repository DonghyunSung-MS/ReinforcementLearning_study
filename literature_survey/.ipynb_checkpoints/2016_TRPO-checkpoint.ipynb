{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION\n",
    "\n",
    "### John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n",
    "#### Ref: [literature](https://arxiv.org/abs/1506.02438)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background(TRPO)\n",
    "\n",
    "This literature starts from Trust Region Policy Optimization(TRPO 2015) and Approximately Opitimal Approximate Reinforcement Learning(2002)\n",
    "\n",
    "### 1.1 TRPO Derivation\n",
    "\n",
    "* Goal Formulation\n",
    "\n",
    "$max imize_{\\theta}\\:[\\nabla_{\\theta}L_{\\theta_{old}}(\\theta)|_{\\theta=\\theta_{old}}(\\theta-\\theta_{old})]$\n",
    "\n",
    "$subject\\:to\\: \\frac{1}{2}(\\theta_{old}-\\theta)^{T}A(\\theta_{old})(\\theta_{old}-\\theta)^{T}\\leq \\delta$\n",
    "\n",
    "$where\\:A(\\theta_{old}) = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}E_{s\\sim\\rho_\\pi}[D_{KL}(\\pi_{\\theta_{old}}||\\pi_{\\theta})]_{\\theta={\\theta_{old}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries\n",
    "- __MDP $(S,A,P,r,\\gamma,\\rho_0)$ :__ \n",
    "\n",
    "where each denotes state, action, transition probability, reward, discount factor, initial state distribution. In here, they set reward as a function of state only and policy as stochastic manner  \n",
    "\n",
    "- __Value Functions :__  \n",
    "\n",
    "$Q_{\\pi}(s_t,a_t) = E_{s_{t+1},a_{t+1}, ...}[\\sum_{l=0}^{\\infty}\\gamma^{l}r(s_{t+l})]$  \n",
    "      \n",
    "$V_{\\pi}(s_t) = E_{a_{t},s_{t+1}, ...}[\\sum_{l=0}^{\\infty}\\gamma^{l}r(s_{t+l})]$\n",
    "<br/>\n",
    "- __Advantage Function :__\n",
    "\n",
    "$A_{\\pi}(s_t,a_t) = Q_{\\pi}(s_t,a_t)-V_{\\pi}(s_t)$  \n",
    "$A_{\\pi}(s,a) = E_{s'\\sim P(s';\\pi)}[r(s)+\\gamma V_{\\pi}(s')-V_{\\pi}(s)]$ \n",
    "<br/>\n",
    "- __Policy Objective :__  \n",
    "It is clear that the agent wants to maximize cummulative reward(return) \n",
    "    * __In time repersentation__\n",
    "$$\\eta(\\pi) = E_{s_0,a_0,s_1,...}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$$\n",
    "    * __In state representation__  \n",
    "$\\eta(\\tilde\\pi) = E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\pi) = E_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi)-E_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big] + E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi)-V_{\\pi}(s_0) + E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + E_{\\tau\\sim\\tilde\\pi}\\Big[-V_{\\pi}(s_0)+\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + E_{\\tau\\sim\\tilde\\pi}\\Big[-V_{\\pi}(s_0)+r(s_0)+\\gamma V_{\\pi}(s_1)+\\gamma\\{-V_{\\pi}(s_1)+r(s_1)+\\gamma V_{\\pi}(s_2)\\}...\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^t A_{\\pi}(s_t,a_t)\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_sP(s_t=s;\\tilde\\pi)\\sum_a\\tilde\\pi(a_t|s_t) A_{\\pi}(s_t,a_t)$  \n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "$$\\large\\eta(\\tilde\\pi) = \\eta(\\pi)+\\sum_{s}\\rho_{\\tilde\\pi}(s)\\sum_{a}\\tilde\\pi(a|s)A_{\\pi}(s,a)$$\n",
    "<br />\n",
    "<br />\n",
    "It is hard to formulate $\\rho_{\\tilde\\pi}(s)$. In general policy iteration method, value function evaluation is followed by policy improvenment. After improvement, the agent has not been experienced or rolled out with new policy so that new discounted unnormalized visitation frequency has not formed yet. The main idea in 2002 and 2015 literature was, instead of using next policy state disturibution(unnormalized), using previous one. \n",
    "<br />\n",
    "<br />\n",
    "$$\\large L_{\\pi}(\\tilde\\pi) = \\eta(\\pi)+\\sum_{s}\\rho_{\\pi}(s)\\sum_{a}\\tilde\\pi(a|s)A_{\\pi}(s,a)$$\n",
    "$$\\large \\pi'\\in argmax_{\\pi'}L_{\\pi}(\\pi')$$\n",
    "$$\\large \\pi_{new}=(1-\\alpha){\\pi_{old}}+\\alpha{\\pi'}$$\n",
    "<br />\n",
    "<br />\n",
    "They suggest that it can not gaurantee direct maximizing the advantage function is equal to improvement in policy. This is because the advantage in practice is parameterized that causes estimation error and approximation error at the same time. Also, they use conservative policy iteration update, for which they could provide explicit lower bounds on the improvement of Î·.\n",
    "<br />\n",
    "<br />\n",
    "- __Boundness__  \n",
    "Let's go with 2002 approach before we dive into 2015 approach which little bit changes in policy improvement.\n",
    "    * __Properties And Condition__  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^t A_{\\pi}(s_t,a_t)\\Big]$  \n",
    "if $\\tilde\\pi=\\pi$  \n",
    "$\\:\\:\\:\\:\\:\\:E_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^t A_{\\pi}(s_t,a_t)\\Big]=0$  \n",
    "$\\:\\:\\:\\:\\:\\:\\sum_{s}\\rho_{\\pi}(s)\\sum_{a}\\pi(a|s)A_{\\pi}(s,a)=0$   \n",
    "$\\:\\:\\:\\:\\:\\:\\sum_{a}\\pi(a|s)A_{\\pi}(s,a)=0$  \n",
    "$\\epsilon_{old} = \\max_s|E_{a\\sim\\pi'}A_{\\pi}(s,a)|\\geq |E_{a\\sim\\pi'}A_{\\pi}(s,a)|$  \n",
    "    * __Derivation__  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + E_{\\tau\\sim\\pi_{new}}\\Big[\\sum_{t=0}^{\\infty}\\gamma^t A_{\\pi}(s_t,a_t)\\Big]$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{s}\\rho_{\\pi_{new}}(s)\\sum_{a}\\pi_{new}(a|s)A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{s}\\rho_{\\pi_{new}}(s)\\sum_{a}\\big\\{(1-\\alpha)\\pi_{old}(a|s)+\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{s}\\rho_{\\pi_{new}}(s)\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_sP(s_t=s;\\pi_{new})\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_s\\big\\{(1-\\alpha)^t P(s_t=s;\\pi_{old\\,only})+\\big(1-(1-\\alpha)^t\\big) P(s_t=s;\\pi_{rest})\\big\\}\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\large Let\\:r_a\\:denotes\\:1-(1-\\alpha)^t$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_s\\big\\{(1-r_a) P(s_t=s;\\pi_{old\\,only})+r_a P(s_t=s;\\pi_{rest})\\big\\}\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = L_{\\pi_{old}}(\\pi_{new}) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_s\\big\\{-r_a P(s_t=s;\\pi_{old\\,only})+r_a P(s_t=s;\\pi_{rest})\\big\\}\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) + \\alpha\\sum_{t=0}^{\\infty}\\gamma^t(-2*r_a*\\epsilon_{old})$ \n",
    "<br />\n",
    "<br />\n",
    "$$\\large\\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) -\\frac{2\\alpha^2\\epsilon_{old}\\gamma}{(1-\\gamma)^2}$$\n",
    "\n",
    "This inequality condition means that if we maximize $L_{\\pi_{old}}(\\pi_{new})$, it gaurantees policy improvements with error term\n",
    "\n",
    "Let's go with 2015 approach. They changes $\\epsilon$ definition and give more generality while having more error term. We denotes $\\epsilon$ as $\\epsilon_{new}$\n",
    "\n",
    "$\\epsilon_{old} = \\max_s|E_{a\\sim\\pi'}A_{\\pi}(s,a)|=\\max_s\\big|\\sum_a\\pi(a|s)A_{\\pi}(s,a)-\\sum_a\\pi'(a|s)A_{\\pi}(s,a)\\big|\\leq 2*\\max_{s,a}|A_{\\pi}(s,a)|=2*\\epsilon_{new}$  \n",
    "<br />\n",
    "<br />\n",
    "$$\\large\\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) -\\frac{4\\alpha^2\\epsilon_{new}\\gamma}{(1-\\gamma)^2}$$\n",
    "$$\\large\\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) -C * D_{KL}^{max}(\\pi_{new}||\\pi_{old}) $$\n",
    "$$where\\,\\, C\\,=\\, \\frac{4\\epsilon_{new}\\gamma}{(1-\\gamma)^2},\\,\\,\\alpha\\,=\\,D_{TV}^{max}(\\pi_{new}||\\pi_{old})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Toward Theory to Practical Implementation form**  \n",
    "    * **Change Boundness to Constant**  \n",
    "In practice, policy($\\pi$) is parameteriezed by $\\theta$. If we follow theoretical step($C$), it would be small. The literature recommands to choose $\\delta$, which changes the optimization problem.\n",
    "$$\\large maximize_{\\theta}\\:L_{\\theta_{old}}(\\theta)$$\n",
    "$$\\large subject\\:to\\:D_{KL}^{max}(\\theta||\\theta_{old})\\leq\\delta$$  \n",
    "    * **Exact method to Heuristic approximation**  \n",
    "It is impractical to calculate $D_{KL}^{max}$ at each iteration. Instead, they choose heuristic approximation($D_{KL}^{\\rho}=E[D_{KL}]$)\n",
    "$$\\large maximize_{\\theta}\\:L_{\\theta_{old}}(\\theta)$$\n",
    "$$\\large subject\\:to\\:D_{KL}^{\\rho}(\\theta||\\theta_{old})\\leq\\delta$$  \n",
    "    * **Expectation becomes Sampled Sum**  \n",
    "In this section, sampled sum which we call Monte-Carlo simulation replaces expecation  \n",
    "$L_{\\pi}(\\pi_{new}) = \\eta(\\pi_{old})+\\sum_{s}\\rho_{\\pi_{old}}(s)\\sum_{a}\\pi_{new}(a|s)A_{\\pi_{old}}(s,a)$  \n",
    "$\\sum_{s}\\rho_{\\pi}(s)\\:\\rightarrow\\:\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{old}}[*]$    \n",
    "$A_{\\pi_{old}}(s,a)\\:\\rightarrow\\:Q_{\\pi_{old}}(s,a)$  \n",
    "Importance of sampling($q$ sampling distribution)   \n",
    "$\\sum_{a}\\pi_{new}(a|s)A_{\\pi_{old}}(s,a)\\:\\rightarrow\\:E_{a\\sim q}\\Big[\\frac{\\pi_{new}}{q}A_{\\pi_{old}}(s,a)\\Big]$\n",
    "$$\\large maximize_{\\theta}\\:E_{a\\sim q,\\,s\\sim \\rho_{old}}\\bigg[\\frac{\\pi_{new}}{q}Q_{\\pi_{old}}(s,a)\\bigg]$$\n",
    "$$\\large subject\\:to\\:D_{KL}^{\\rho}(\\theta||\\theta_{old})\\leq\\delta$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
