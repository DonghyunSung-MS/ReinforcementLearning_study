{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION\n",
    "\n",
    "### John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n",
    "#### Ref: [literature](https://arxiv.org/abs/1506.02438)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background(TRPO)\n",
    "\n",
    "This literature starts from Trust Region Policy Optimization(TRPO 2015) and Approximately Opitimal Approximate Reinforcement Learning(2002)\n",
    "\n",
    "### 1.1 TRPO Derivation\n",
    "\n",
    "* Goal Formulation\n",
    "\n",
    "$max imize_{\\theta}\\:[\\nabla_{\\theta}L_{\\theta_{old}}(\\theta)|_{\\theta=\\theta_{old}}(\\theta-\\theta_{old})]$\n",
    "\n",
    "$subject\\:to\\: \\frac{1}{2}(\\theta_{old}-\\theta)^{T}A(\\theta_{old})(\\theta_{old}-\\theta)^{T}\\leq \\delta$\n",
    "\n",
    "$where\\:A(\\theta_{old}) = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}E_{s\\sim\\rho_\\pi}[D_{KL}(\\pi_{\\theta_{old}}||\\pi_{\\theta})]_{\\theta={\\theta_{old}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries\n",
    "- __MDP $(S,A,P,r,\\gamma,\\rho_0)$ :__ \n",
    "\n",
    "where each denotes state, action, transition probability, reward, discount factor, initial state distribution. In the TRPO paper, the reward is defined as a single-variable function that soley considers the respective state and the policy is defined as a stochastic one.\n",
    "\n",
    "- __Value Functions :__  \n",
    "\n",
    "$Q_{\\pi}(s_t,a_t) = E_{s_{t+1},a_{t+1}, ...}[\\sum_{l=0}^{\\infty}\\gamma^{l}r(s_{t+l})]$  \n",
    "      \n",
    "$V_{\\pi}(s_t) = E_{a_{t},s_{t+1}, ...}[\\sum_{l=0}^{\\infty}\\gamma^{l}r(s_{t+l})]$\n",
    "<br/>\n",
    "- __Advantage Function :__\n",
    "\n",
    "$A_{\\pi}(s_t,a_t) = Q_{\\pi}(s_t,a_t)-V_{\\pi}(s_t)$  \n",
    "$A_{\\pi}(s,a) = E_{s'\\sim P(s';\\pi)}[r(s)+\\gamma V_{\\pi}(s')-V_{\\pi}(s)]$ \n",
    "<br/>\n",
    "- __Policy Objective :__  \n",
    "It is clear that the agent wants to maximize cummulative reward(return) \n",
    "    * __In time repersentation__\n",
    "$$\\eta(\\pi) = E_{s_0,a_0,s_1,...}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$$\n",
    "    * __In state representation__  \n",
    "$\\eta(\\tilde\\pi) = E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\pi) = E_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi)-E_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big] + E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi)-V_{\\pi}(s_0) + E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + E_{\\tau\\sim\\tilde\\pi}\\Big[-V_{\\pi}(s_0)+\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + E_{\\tau\\sim\\tilde\\pi}\\Big[-V_{\\pi}(s_0)+r(s_0)+\\gamma V_{\\pi}(s_1)+\\gamma\\{-V_{\\pi}(s_1)+r(s_1)+\\gamma V_{\\pi}(s_2)\\}...\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^t A_{\\pi}(s_t,a_t)\\Big]$  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_sP(s_t=s;\\tilde\\pi)\\sum_a\\tilde\\pi(a_t|s_t) A_{\\pi}(s_t,a_t)$  \n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "$$\\large\\eta(\\tilde\\pi) = \\eta(\\pi)+\\sum_{s}\\rho_{\\tilde\\pi}(s)\\sum_{a}\\tilde\\pi(a|s)A_{\\pi}(s,a)$$\n",
    "<br />\n",
    "<br />\n",
    "It is hard to formulate $\\rho_{\\tilde\\pi}(s)$. In general policy iteration method, value function evaluation is followed by policy improvenment. After improvement, the agent has not been experienced or rolled out with new policy so that new discounted unnormalized visitation frequency has not formed yet. The main idea in 2002 and 2015 literature was, instead of using next policy state disturibution(unnormalized), using previous one. \n",
    "<br />\n",
    "<br />\n",
    "$$\\large L_{\\pi}(\\tilde\\pi) = \\eta(\\pi)+\\sum_{s}\\rho_{\\pi}(s)\\sum_{a}\\tilde\\pi(a|s)A_{\\pi}(s,a)$$\n",
    "$$\\large \\pi'\\in argmax_{\\pi'}L_{\\pi}(\\pi')$$\n",
    "$$\\large \\pi_{new}=(1-\\alpha){\\pi_{old}}+\\alpha{\\pi'}$$\n",
    "<br />\n",
    "<br />\n",
    "They suggest that it can not gaurantee direct maximizing the advantage function is equal to improvement in policy. This is because the advantage in practice is parameterized that causes estimation error and approximation error at the same time. Also, they use conservative policy iteration update, for which they could provide explicit lower bounds on the improvement of Î·.\n",
    "<br />\n",
    "<br />\n",
    "- __Boundness__  \n",
    "Let's go with 2002 approach before we dive into 2015 approach which little bit changes in policy improvement.\n",
    "    * __Properties And Condition__  \n",
    "$\\eta(\\tilde\\pi) = \\eta(\\pi) + E_{\\tau\\sim\\tilde\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^t A_{\\pi}(s_t,a_t)\\Big]$  \n",
    "if $\\tilde\\pi=\\pi$  \n",
    "$\\:\\:\\:\\:\\:\\:E_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^t A_{\\pi}(s_t,a_t)\\Big]=0$  \n",
    "$\\:\\:\\:\\:\\:\\:\\sum_{s}\\rho_{\\pi}(s)\\sum_{a}\\pi(a|s)A_{\\pi}(s,a)=0$   \n",
    "$\\:\\:\\:\\:\\:\\:\\sum_{a}\\pi(a|s)A_{\\pi}(s,a)=0$  \n",
    "$\\epsilon_{old} = \\max_s|E_{a\\sim\\pi'}A_{\\pi}(s,a)|\\geq |E_{a\\sim\\pi'}A_{\\pi}(s,a)|$  \n",
    "    * __Derivation__  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + E_{\\tau\\sim\\pi_{new}}\\Big[\\sum_{t=0}^{\\infty}\\gamma^t A_{\\pi}(s_t,a_t)\\Big]$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{s}\\rho_{\\pi_{new}}(s)\\sum_{a}\\pi_{new}(a|s)A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{s}\\rho_{\\pi_{new}}(s)\\sum_{a}\\big\\{(1-\\alpha)\\pi_{old}(a|s)+\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{s}\\rho_{\\pi_{new}}(s)\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_sP(s_t=s;\\pi_{new})\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_s\\big\\{(1-\\alpha)^t P(s_t=s;\\pi_{old\\,only})+\\big(1-(1-\\alpha)^t\\big) P(s_t=s;\\pi_{rest})\\big\\}\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\large Let\\:r_a\\:denotes\\:1-(1-\\alpha)^t$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = \\eta(\\pi_{old}) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_s\\big\\{(1-r_a) P(s_t=s;\\pi_{old\\,only})+r_a P(s_t=s;\\pi_{rest})\\big\\}\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) = L_{\\pi_{old}}(\\pi_{new}) + \\sum_{t=0}^{\\infty}\\gamma^t\\sum_s\\big\\{-r_a P(s_t=s;\\pi_{old\\,only})+r_a P(s_t=s;\\pi_{rest})\\big\\}\\sum_{a}\\big\\{\\alpha\\pi'(a|s)\\big\\}A_{\\pi_{old}}(s,a)$  \n",
    "$\\:\\:\\:\\:\\:\\:\\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) + \\alpha\\sum_{t=0}^{\\infty}\\gamma^t(-2*r_a*\\epsilon_{old})$ \n",
    "<br />\n",
    "<br />\n",
    "$$\\large\\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) -\\frac{2\\alpha^2\\epsilon_{old}\\gamma}{(1-\\gamma)^2}$$\n",
    "\n",
    "This inequality condition means that if we maximize $L_{\\pi_{old}}(\\pi_{new})$, it gaurantees policy improvements with error term\n",
    "\n",
    "Let's go with 2015 approach. They changes $\\epsilon$ definition and give more generality while having more error term. We denotes $\\epsilon$ as $\\epsilon_{new}$\n",
    "\n",
    "$\\epsilon_{old} = \\max_s|E_{a\\sim\\pi'}A_{\\pi}(s,a)|=\\max_s\\big|\\sum_a\\pi(a|s)A_{\\pi}(s,a)-\\sum_a\\pi'(a|s)A_{\\pi}(s,a)\\big|\\leq 2*\\max_{s,a}|A_{\\pi}(s,a)|=2*\\epsilon_{new}$  \n",
    "<br />\n",
    "<br />\n",
    "$$\\large\\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) -\\frac{4\\alpha^2\\epsilon_{new}\\gamma}{(1-\\gamma)^2}$$\n",
    "$$\\large\\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) -C * D_{KL}^{max}(\\pi_{new}||\\pi_{old}) $$\n",
    "$$where\\,\\, C\\,=\\, \\frac{4\\epsilon_{new}\\gamma}{(1-\\gamma)^2},\\,\\,\\alpha\\,=\\,D_{TV}^{max}(\\pi_{new}||\\pi_{old})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Toward Theory to Practical Implementation form**  \n",
    "    * **Change Boundness to Constant**  \n",
    "In practice, policy($\\pi$) is parameteriezed by $\\theta$. If we follow theoretical step($C$), it would be small. The literature recommands to choose $\\delta$, which changes the optimization problem.\n",
    "$$\\large maximize_{\\theta}\\:L_{\\theta_{old}}(\\theta)$$\n",
    "$$\\large subject\\:to\\:D_{KL}^{max}(\\theta||\\theta_{old})\\leq\\delta$$  \n",
    "    * **Exact method to Heuristic approximation**  \n",
    "It is impractical to calculate $D_{KL}^{max}$ at each iteration. Instead, they choose heuristic approximation($D_{KL}^{\\rho}=E[D_{KL}]$)\n",
    "$$\\large maximize_{\\theta}\\:L_{\\theta_{old}}(\\theta)$$\n",
    "$$\\large subject\\:to\\:D_{KL}^{\\rho}(\\theta||\\theta_{old})\\leq\\delta$$  \n",
    "    * **Expectation becomes Sampled Sum**  \n",
    "In this section, sampled sum which we call Monte-Carlo simulation replaces expecation  \n",
    "$L_{\\pi}(\\pi_{new}) = \\eta(\\pi_{old})+\\sum_{s}\\rho_{\\pi_{old}}(s)\\sum_{a}\\pi_{new}(a|s)A_{\\pi_{old}}(s,a)$  \n",
    "$\\sum_{s}\\rho_{\\pi}(s)\\:\\rightarrow\\:\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{old}}[*]$    \n",
    "$A_{\\pi_{old}}(s,a)\\:\\rightarrow\\:Q_{\\pi_{old}}(s,a)$  \n",
    "Importance of sampling($q$ sampling distribution)   \n",
    "$\\sum_{a}\\pi_{new}(a|s)A_{\\pi_{old}}(s,a)\\:\\rightarrow\\:E_{a\\sim q}\\Big[\\frac{\\pi_{new}}{q}A_{\\pi_{old}}(s,a)\\Big]$\n",
    "$$\\large maximize_{\\theta}\\:E_{a\\sim q,\\,s\\sim \\rho_{old}}\\bigg[\\frac{\\pi_{new}}{q}Q_{\\pi_{old}}(s,a)\\bigg]$$\n",
    "$$\\large subject\\:to\\:D_{KL}^{\\rho}(\\theta||\\theta_{old})\\leq\\delta$$\n",
    "    * **Linear Approximation of Objective Function, Quadratic Approximation of Constraint**  \n",
    "proof -> scroll down    \n",
    "$$\\large maximize_{\\theta}\\:\\nabla L \\Delta\\theta$$\n",
    "$$\\large subject\\:to\\:\\frac{1}{2}\\Delta\\theta^T A \\Delta\\theta\\leq\\delta$$\n",
    "$$where,\\:A\\,is\\,Fisherman Information Matrix(FIM),\\,also\\,Hessian\\,of\\,KL$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Implemented form of TRPO\n",
    "<br />\n",
    "\n",
    "$$maximize_{\\theta}\\:\\nabla L \\Delta\\theta$$\n",
    "  \n",
    "$$subject\\:to\\:\\frac{1}{2}\\Delta\\theta^T A \\Delta\\theta\\leq\\delta$$\n",
    "### 1. Lagragian Method\n",
    "$F(\\Delta\\theta,\\,\\lambda)\\,=\\,\\nabla L^T\\,\\Delta\\theta\\,-\\,\\lambda\\big(\\frac{1}{2}\\Delta\\theta^TA \\Delta\\theta\\,-\\,\\delta\\big) $  \n",
    "<br />\n",
    "$\\nabla F(\\Delta\\theta,\\,\\lambda)=0$  \n",
    "<br />\n",
    "This yields to two equation  \n",
    "<br />\n",
    "$\\nabla L - \\lambda A \\Delta\\theta=0$  ---- (1)  \n",
    "$\\Delta\\theta=\\frac 1 \\lambda A^{-1}\\nabla L$  \n",
    "<br />\n",
    "$\\lambda(\\frac{1}{2}\\Delta\\theta^T A \\Delta\\theta-\\delta)$  ---- (2)  \n",
    "<br />\n",
    "By KKT condition, $\\lambda$ should be positive and $\\delta$ should be $\\frac{1}{2}\\Delta\\theta^T A \\Delta\\theta$.  \n",
    "<br />\n",
    "Use second condition and equation 1  \n",
    "<br />\n",
    "$\\delta=\\frac{1}{2}\\Delta\\theta^T A \\Delta\\theta=\\frac{1}{2\\lambda^2}\\nabla L^T A^{-1}\\nabla L$  \n",
    "<br />\n",
    "$\\frac{1}{\\lambda} = \\large\\sqrt{\\frac{2\\delta}{\\nabla L^T A^{-1}\\nabla L}}$  \n",
    "<br />\n",
    "Finally, we get update law and maximum value\n",
    "<br />\n",
    "\n",
    "$\\Delta\\theta = \\theta-\\theta_{old}= \\large{\\sqrt{\\frac{2\\delta}{\\nabla L^T A^{-1}\\nabla L}}}A^{-1}\\nabla L $  \n",
    "$A^{-1}\\nabla L $ denotes __search direction__.\n",
    "${\\sqrt{\\frac{2\\delta}{\\nabla L^T A^{-1}\\nabla L}}}$ indicates __step size__.\n",
    "<br />  \n",
    "<br />  \n",
    "$ \\nabla L \\Delta\\theta = \\large{\\sqrt{\\frac{2\\delta}{\\nabla L^T A^{-1}\\nabla L}}}{\\nabla L^T A^{-1}\\nabla L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conjugate Gradient Method\n",
    "<br />\n",
    "This is iterative method to solve $Ax=b$ or $minimize_{x}\\: \\Phi(x)=\\frac{1}{2}x^TAx-b^Tx$\n",
    "<br />\n",
    "where, $A$ is positive definite.  \n",
    "<br />\n",
    "<br />\n",
    "In previous section, we need $A^{-1}\\nabla L$ to calculate update law. However, it is comuputationally high to get inverse of hessian($A^{-1}$). Instead, we compute hessian vector product iteratively.\n",
    "<br />\n",
    "<br />\n",
    "In this case $Ax=g$, where $g$ is gradient of objective function($\\nabla L$)  \n",
    "\n",
    "#### Derivation([One-Minute Derivation of The Conjugate Gradient Algorithm](https://arxiv.org/abs/1608.08691))\n",
    "\n",
    "we want to solve for $Ax^*=g$  \n",
    "<br />\n",
    "$x_{i+1}=x_{i}+\\alpha_i d_i$  \n",
    "$x_{i+1}-x^*= x_{i}-x^*+\\alpha_i d_i$  \n",
    "<br />\n",
    "Let, $e_i=x_i-x^*$  \n",
    "<br />\n",
    "$e_{i+1} = e_{i}+\\alpha_i d_i$  \n",
    "$Ae_{i+1} = Ae_{i}+\\alpha_i Ad_i$  \n",
    "<br />\n",
    "Let, $r_i=g-Ax_i$  \n",
    "<br />\n",
    "$r_{i+1} = r_{i}-\\alpha_i Ad_i$  \n",
    "<br />\n",
    "Let, we choose next residual perpendiculart to previous one $r_{i+1}^Tr_i=r_{i}^Tr_{i+1}=0$.  \n",
    "<br />\n",
    "$r_{i}^Tr_{i+1} = r_{i}^Tr_{i}-\\alpha_i\\, r_{i}^TAd_i=0$  \n",
    "$$\\alpha_i = \\frac{r_{i}^Tr_{i}}{r_{i}^TAd_i}$$  \n",
    "\n",
    "We have to calculate $d_i$ iteratively. \n",
    "\n",
    "In fact, another approch to explain this part is A-conjugate or A-orthogonal. Vector $d_i$ is n-dimensinal, which is basis of A. You can find Gram Schmidt Orthogonalization if you need more information. Briefly, we can build n orthogonal basis vectors from any vector $d_0$ if A is invertible(non-zero eigen values). \n",
    "\n",
    "$$d_{i}^TAd_{i+1}=0$$  \n",
    "Let\n",
    "$d_{i+1} = r_{i+1} + \\beta_{i+1}d_{i}$  \n",
    "<br />\n",
    "$d_{i}^TAd_{i+1} = d_{i}^TAr_{i+1} + \\beta_{i+1}d_{i}^TAd_{i}$   \n",
    "$0 = d_{i}^TAr_{i+1} + \\beta_{i+1}d_{i}^TAd_{i}$  \n",
    "$$\\beta_{i+1} = -\\frac{d_{i}^TAr_{i+1}}{d_{i}^TAd_{i}}$$\n",
    "<br />\n",
    "$r_{i+1}^T = r_{i}^T-\\alpha_id_i^T A\\,\\,\\,$  _notes_. A is symetic because it is hessian matrix   \n",
    "$r_{i+1}^Tr_{i+1} = r_{i}^Tr_{i+1}-\\alpha_id_i^T Ar_{i+1}\\,\\,\\,$   \n",
    "$d_i^T Ar_{i+1}=-\\frac{1}{\\alpha_i}r_{i+1}^Tr_{i+1}\\,\\,\\,$   \n",
    "<br />\n",
    "$d_{i} = r_{i} + \\beta_{i}d_{i-1}$  \n",
    "$Ad_{i} = Ar_{i} + \\beta_{i}Ad_{i-1}$  \n",
    "$d_{i}^TAd_{i} = d_{i}^TAr_{i} + \\beta_{i}d_{i}^TAd_{i-1}$  \n",
    "$d_{i}^TAd_{i} = d_{i}^TAr_{i}$   \n",
    "$$\\beta_{i+1} = -\\frac{d_{i}^TAr_{i+1}}{d_{i}^TAd_{i}} = \\frac{1}{\\alpha_i}\\frac{r_{i+1}^Tr_{i+1}}{d_{i}^TAd_{i}}= \\frac{r_{i+1}^Tr_{i+1}}{{r_{i}^Tr_{i}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "1. Initial guess $x_0$  \n",
    "1. Set initial condition $d_0=r_0=b-Ax_0$  \n",
    "1. Repeat until $r_0$ is sufficiently small(gradient of objective is near zero or linear system is near solution) \n",
    "    * compute $$\\alpha_i = \\frac{r_{i}^Tr_{i}}{r_{i}^TAd_i}$$\n",
    "    * update$$x_{i+1}=x_{i}+\\alpha_i d_i$$  \n",
    "    * update $$r_{i+1} = r_{i}-\\alpha_i Ad_i$$\n",
    "    * compute $$\\beta_{i+1} = \\frac{r_{i+1}^Tr_{i+1}}{{r_{i}^Tr_{i}}}$$\n",
    "    * update $$d_{i+1} = r_{i+1} + \\beta_{i+1}d_{i}$$   \n",
    "1. Return $x_n$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to compute $Ad_i$\n",
    "<br />\n",
    "We use conjugate gradient method to avoid caculating inverse of hessian matrix. However, there is one problem left. We need A matrix in algorithm, which is KL divergence of hessian. It is well-know that caculating gradient is computationally cheap than getting hessian itself. Therfore, we use under equation.  \n",
    "<br />  \n",
    "<br />\n",
    "\n",
    "$$\\nabla^2 KL\\: d_i = \\nabla(\\nabla KL\\: d_i) $$ \n",
    "<br />\n",
    "<center >\n",
    "where $\\nabla^2 KL$ is $A$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be specific  \n",
    "1. Get $KL(\\pi,\\pi_{old})$\n",
    "1. Use auto grad package to get $\\nabla KL(\\pi,\\pi_{old})$\n",
    "1. Get $\\nabla KL(\\pi,\\pi_{old})d_{i}$\n",
    "1. Again, use auto grad package to get  $\\nabla(\\nabla KL(\\pi,\\pi_{old})d_{i})$  \n",
    "<br />\n",
    "\n",
    "Now, you get $Ad_i$ indirectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summarry\n",
    "<br />\n",
    "\n",
    "1. Compute search direction($A^{-1}\\,\\nabla L$) using conjugate gradient method(n-step)\n",
    "    * Input : initial guess of $x_0$ ex. zero vector and gradient of objective function\n",
    "    * In algorithm, use hessian vector prouct to get $Ad_i$ indirecty\n",
    "1. Get step size Using equation\n",
    "<br />  \n",
    "$$\\sqrt{\\frac{2\\delta}{\\nabla L^T A^{-1}\\nabla L}}$$\n",
    "<br />  \n",
    "$$\\sqrt{\\frac{2\\delta}{{\\big(A (A^{-1}\\nabla L})\\big)^T(A^{-1}\\nabla L)}}$$\n",
    "1. Update parmeter Vector($\\theta$)\n",
    "<br />  \n",
    "$$\\theta= \\theta_{old}+{\\sqrt{\\frac{2\\delta}{\\nabla L^T A^{-1}\\nabla L}}}A^{-1}\\nabla L$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
